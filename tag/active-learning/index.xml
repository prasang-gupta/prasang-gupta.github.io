<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Active Learning | Prasang Gupta</title>
    <link>https://prasang-gupta.github.io/tag/active-learning/</link>
      <atom:link href="https://prasang-gupta.github.io/tag/active-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Active Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Prasang Gupta</copyright><lastBuildDate>Sun, 01 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://prasang-gupta.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Active Learning</title>
      <link>https://prasang-gupta.github.io/tag/active-learning/</link>
    </image>
    
    <item>
      <title>Contract Lifecycle Management (CLM)</title>
      <link>https://prasang-gupta.github.io/project/contractlifecyclemgmnt/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/contractlifecyclemgmnt/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;PROBLEM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The client wanted to switch from legacy contract management tool to a new vendor. However, they were not confident of the metadata they had for the contracts and wanted help in getting all the major information extracted from the documents. They also wanted hierarchical linkages for most of the contracts for better organisation in the new tool.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;SOLUTION&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We solved the first problem with a myriad of different tools. These included advanced NLP models for entity extraction based on transformers, tree based ML models for certain classifications and logic-driven string search models.&lt;/p&gt;
&lt;p&gt;Common fields that are present in almost every document were extracted using a trained NER model linked with an active learning pipeline. These models were built using a modified version of &lt;a href=&#34;https://github.com/princeton-nlp/PURE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PURE&lt;/a&gt;, an open source offering by Princeton NLP. The active learning pipeline was built using LabelStudio as a frontend for manual annotations to get initial training data and then subsequently, for getting labels for the most uncertain samples.&lt;/p&gt;
&lt;p&gt;Fields which had only a few possible values like &amp;ldquo;Agreement Type&amp;rdquo;, were trained using a simple Random Forest classifier. The training data was obhtained using a mix of string extractions from the filenames and the existing contract metadata provided by the client. This, combined with the NER model resulted in boosted accuracies for this field.&lt;/p&gt;
&lt;p&gt;We built logic-based string search models for those fields which were not abundantly present and were more situational. These included fields like &amp;ldquo;Force Majeure&amp;rdquo; and all fields associated with this, &amp;ldquo;Termination Payments&amp;rdquo;, etc. This helped us extract information for these fields for the majority of the documents they were present in with little overhead.&lt;/p&gt;
&lt;p&gt;All of these extractions were provided to the client and after spot-checking and QA, were used in the construction of hierarchical linkages. Strong linkages were built by extracting the reference phrase within contracts that contains the details regarding the parent or the child contract. These were then translated back to the database we had. Weak linkages were also built based off of logic to combine documents under each MSA umbrella wherever the extraction of the reference phrase wasn&amp;rsquo;t possible.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;IMPACT&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The extractions provided to the client passed the QA test with about 90% accuracy. Also, we managed to provide hierchical linkages for more than 75% of the documents provided to us that could be linked.&lt;/p&gt;
&lt;p&gt;This has historically been a manual-only job with several hours of manpower invested in reading through the huge contracts and extracting information. We accelerated this process by appending this with models wherever possible. This reduced the load on the manual annotators and we were able to complete this in a fraction of the time that would&amp;rsquo;ve been invested in an all-manual project &lt;span style=&#34;color:#5DADE2;font-style:bold&#34;&gt;saving roughly 30,000 hours and $750,000 for the client&lt;/span&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Demand Prediction with Competition Analysis</title>
      <link>https://prasang-gupta.github.io/project/demandprediction/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/demandprediction/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;AIM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this study was to predict consumer demand using data from competition and infusing it with other datasets that influence demand like mobility and demographic variables. The other goal was to correct a proprietary mobility dataset used by PwC to incorporate for mobility changes brought on by COVID restrictions.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;DETAILS&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proprietary dataset mentioned had several flaws including data inconsistencies over months and years and noisy data which was not helping in developing stable models. It also didn&amp;rsquo;t account for any socio-economic variables based on the demographics of the region. To correct this, the proprietary dataset was merged with demographics variables based on the location and some feature engineering was done to get the mobility in radii of 1km to 5km based on the store location. The final sales value was kept as the dependent field for the model.&lt;/p&gt;
&lt;p&gt;Multiple model architectures were tried to model this information to correct the mobility, including ML and DL models. Finally, a voting ensemble method was selected which was modelling the corrected demand with an accuracy of about 70%.&lt;/p&gt;
&lt;p&gt;To bump up the numbers further, an aerial snapshot of the region around the score was obtained from Google Satellite API at a tuned zoom level and a segmentation exercise was performed on the image to extract the type of region around the store. This was expected to bring another dimension of immediate demographics in the dataset. This was done by extracting the coverage area of different colors based on pixel masks :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grey (signifying roads, parking lots, streets, etc)&lt;/li&gt;
&lt;li&gt;Green (signifying agriculture, parks, vegetation, etc)&lt;/li&gt;
&lt;li&gt;White (signifying roof-tops, buildings, etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;IMPACT&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The final model with all the variables included (engineered visits from proprietary data, Google mobility data, demographics data and derived immediate demographics of the store from satellite images) achieved a test accuracy of 75%. This improved the quality of the mobility data and impacted tens of projects that utilised this mobility dataset helping in building much more accurate and robust models for client deliveries.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active Learning</title>
      <link>https://prasang-gupta.github.io/project/activelearning/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/activelearning/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;AIM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of the study was to build out a pipeline for active learning that other projects with scarce labelled data will leverage for model building. It also included trying the effectiveness of active learning on a standard dataset and evaluate different techniques.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;DETAILS&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset chosen for this study was CIFAR10. This dataset contains 60,000 32x32 images with 10 unique classes. We used a maximum of 10,000 images at each time for training purposes. This study&amp;rsquo;s focus was to compare the performance of active and passive learning using same number of labelled samples.&lt;/p&gt;
&lt;p&gt;The passive learning model would be trained on a uniform distribution of the dataset size, however, the active learning model would be initialised with half the training size and then would be trained for the full size by sequential queries. We tried 3 queries in this study : uncertainty, margin sampling and entropy sampling. We achieved better performance with active learning for all the sizes, however, the improvement in performance varied from as low as 1% to a sizable 23%. The details for all the experiments and the queries can be seen in the slides.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;IMPACT&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The pipeline developed from this project was consecutively used in multiple client projects to improve the modelling capabilities and thrive wherever manually labelling the full dataset was not an option.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hackathon | Prasang Gupta</title>
    <link>https://prasang-gupta.github.io/tag/hackathon/</link>
      <atom:link href="https://prasang-gupta.github.io/tag/hackathon/index.xml" rel="self" type="application/rss+xml" />
    <description>Hackathon</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Prasang Gupta</copyright><lastBuildDate>Wed, 01 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://prasang-gupta.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Hackathon</title>
      <link>https://prasang-gupta.github.io/tag/hackathon/</link>
    </image>
    
    <item>
      <title>HTML UI element extraction</title>
      <link>https://prasang-gupta.github.io/project/websiteuidetection/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/websiteuidetection/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$2^{nd}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this hackathon was to localise and identify several different HTML UI elements in hand-drawn wireframe drawings of websites as well as for screenshots of real websites.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset provided for the hackathon contained about 3200 wireframe drawings of websites. The goal was to identify the different HTML UI elemnents present in the image, such as &amp;ldquo;Text Box&amp;rdquo;, &amp;ldquo;Button&amp;rdquo;, &amp;ldquo;Image&amp;rdquo;, etc. Hence, it boiled down to an object detection problem. It also included another dataset containing screenshots of real websites. The problem remained the same for both the datasets.&lt;/p&gt;
&lt;p&gt;We used several different Object Detection techniques and decided on using the just released SOTA model YOLOv5. We tried different flavours of YOLOv5 and since inference time was not a bar, we went ahead with the XL version of the same to boost performance. We also used pre-trained weights and performed a LR scheduler study to boost the scores even further.&lt;/p&gt;
&lt;p&gt;We also observed that our model was giving out good predictions for the common elements with high confidences, but was not giving outputs for the more uncommon elements. Hence, we performed a study to change the confidence cutoff levels to optimise it for the use case and adjust it according to the distribution in the data. This allowed us to achieve near-perfect performance level of 0.95 F1 score on the validation set.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution built was performing really good on unseen test images managing an mAP value of 0.82. This model was later swapped with the last year&amp;rsquo;s model in the already developed pipeline to allow rapid prototyping of websites and dashboards.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Twitter Bias Hackathon</title>
      <link>https://prasang-gupta.github.io/project/twitterbiashack/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/twitterbiashack/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$9^{th}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of the hackathon was to higlight bias in Twitter&amp;rsquo;s Saliency model.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Twitter&amp;rsquo;s Saliency model is a model that is used to crop over-sized images to fit on the screen representing a thumbnail preview. The saliency model is responsible for selecting the most &amp;ldquo;salient&amp;rdquo; part of the image and that part is consequently kept in the thumbnail and the area surroinding it is cropped off.&lt;/p&gt;
&lt;p&gt;We decided to chose, recently concluded at the time, Tokyo Olympics 2022 as a case study for highlighting bias. We downloaded around 5000 images across 10 different sports and ran the saliency model on top of them. We also passed these images through an object detection model to identify images with and without athletes.&lt;/p&gt;
&lt;p&gt;We ran several studies, the main being classifying the images and the crops as something that was biased or not-biased. We also ran a sensitivity analysis and also tried observing changes between original coloured images and an image with a filter put on it (sepia and grayscale).&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We found out that the model, in some cases, was biased towards text present in the image (which was mostly Tokyo 2020). We also found in several cases, where the main focus of the image, the athletes were not the ones selected as the most salient in the image, instead, the saliency model was predicting either someone from the audience or billboards / advertisements as the most salient. This report was submitted to Twitter and we got a thanks from the Twitter team for highlighting this bias in their model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L2RPN Hackathon 2020 - Robustness Track</title>
      <link>https://prasang-gupta.github.io/project/l2rpn/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/l2rpn/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$28^{th}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt; (&lt;span style=&#34;color:#33cc33;font-style:bold;font-size:100%&#34;&gt; $21^{st}$ highest score&lt;/span&gt; )&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of L2RPN (Learning to Run a Power Network) 2020 hackathon - Robustness Track was to build a robust agent that would be keep delivering reliable electricity everywhere and also operate the grid safely when an agent takes unknown adversarial actions at regular intervals.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset contained episodes spanning different adversarial actions taken by the agent. The agent could terminate any 1 of the 10 possible power lines (some of them being high voltage lines). Our agent was to be evaluated on how long it can provide reliable power to consumers without causing blackout. Once a blackout occurs, it is game over for that episode. The operation cost to be minimized included powerline losses, redispatch cost and blackout cost.&lt;/p&gt;
&lt;p&gt;Every substation in the competition grid had a &amp;ldquo;double busbar layout&amp;rdquo;. Hence, there was a choice of bus for making a connection from one of the bus to a grid object. Due to this and the medium sized grid used, the action space was of the order of 10^5 with the combinatorial action space reaching infinity.&lt;/p&gt;
&lt;p&gt;Our approach was derived from a previous &lt;a href=&#34;https://github.com/ZM-Learn/L2RPN_WCCI_a_Solution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;public solution&lt;/a&gt; for this competition. THe idea was to reduce the action space and train 2 A3C models with different training parameters.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We managed to improve upon the baseline set and managed to achieve a score of 10.84, which was the $21^{st}$ highest score on the leaderboard, gaining us $28^{th}$ rank.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HTML UI element extraction</title>
      <link>https://prasang-gupta.github.io/project/wireframeuidetection/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/wireframeuidetection/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$3^{rd}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this hackathon was to localise and identify several different HTML UI elements in hand-drawn wireframe drawings of websites.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset provided for the hackathon contained about 3000 wireframe drawings of websites. The goal was to identify the different HTML UI elemnents present in the image, such as &amp;ldquo;Text Box&amp;rdquo;, &amp;ldquo;Button&amp;rdquo;, &amp;ldquo;Image&amp;rdquo;, etc. Hence, it boiled down to an object detection problem.&lt;/p&gt;
&lt;p&gt;We tried using several Object Detection algorithms like YOLO, R-CNN and Mask-RCNN and decided on Mask-RCNN as it was providing us with the best results. However, one thing we observed in our outputs was that our Precision scores were good, but the model was lacking in Recall bringing the whole F1 down. To solve this problem, we came up with a novel technique &amp;ldquo;Multi-Pass Inference&amp;rdquo; that booosted our recall scores.&lt;/p&gt;
&lt;p&gt;The technique involves running the image through the model multiple times, each time taking note of the objects that are already detected and removing them for subsequent passes. This forced the model to predict more instances of the elements in the image. We smarlty combined the objects detected in multiple passes to overall boost the recall score of our model helping us to take a podium spot in the leaderboard.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution built was performing really good on unseen test images managing an mAP (IoU &amp;gt; 0.5) score of 64.12. This solution was later implemented into a pipeline to allow rapid prototyping of websites and dashboards.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

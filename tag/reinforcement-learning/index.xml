<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning | Prasang Gupta</title>
    <link>https://prasang-gupta.github.io/tag/reinforcement-learning/</link>
      <atom:link href="https://prasang-gupta.github.io/tag/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Reinforcement Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Prasang Gupta</copyright><lastBuildDate>Thu, 24 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://prasang-gupta.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Reinforcement Learning</title>
      <link>https://prasang-gupta.github.io/tag/reinforcement-learning/</link>
    </image>
    
    <item>
      <title>Intelligent Systematic Investment Agent: an ensemble of deep learning and evolutionary strategies</title>
      <link>https://prasang-gupta.github.io/publication/rlisia/</link>
      <pubDate>Thu, 24 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/publication/rlisia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Intelligent Systematic Investment Agent</title>
      <link>https://prasang-gupta.github.io/project/rlisia/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/rlisia/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;AIM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to explore Evolutionary strategies as an alternative to the classicaly used Reinforcement learning techniques to solve the ETF trading investment problem.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;DETAILS&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Historically, autonomous agents for trading and investment are generally built using one of the many reinforcement learning techniques. These include the SOTA Actor-critic algorithms as well as simpler DQN based algorithms. The problem with these is that an environment needs to be setup for training them and the learning curve for these algorithms is a bit steep for someone inexperienced in the field of Reinforcement Learning.&lt;/p&gt;
&lt;p&gt;We demonstrated that using Evolutionary strategies to solve episodic problems (which is getting daily purchase actions for a month in the current problem) can be a much easier and robust method. We also coupled this with a simple Neural network model to learn the patterns that are being revealed in the outputs of the ES runs. This broke down the problem into 2 mutually exclusive parts, which is much easier to understand and code.&lt;/p&gt;
&lt;p&gt;We compared both qualitative and quantitative metrics for our ensemble algorithm we call &lt;code&gt;GADLE&lt;/code&gt; with 2 traditional RL-based solutions, Actor-critic and DQN. Qualitatively, we compared the ease of writing of code and ease of understanding. Quantitatively, we ran different experiments centered around performance, sensitivity and consistency of the solutions.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;IMPACT&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proposed GADLE algorithm performs at par with the Actor-critic algorithm. However, it crushes the competition in terms of consistency and sensitivity. To put things in perspective, following are the tables summarising results of the sensitivity and the consistency experiments.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;sensitivity.png&#34; alt=&#34;Sensitivity comparison&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;consistency.png&#34; alt=&#34;Consistency comparison&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L2RPN Hackathon 2020 - Robustness Track</title>
      <link>https://prasang-gupta.github.io/project/l2rpn/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/l2rpn/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#58D68D&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#52BE80;font-style:bold;font-size:120%&#34;&gt;28th rank&lt;/span&gt; &lt;span style=&#34;color:#58D68D&#34;&gt;in the hackathon.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;AIM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of L2RPN (Learning to Run a Power Network) 2020 hackathon - Robustness Track was to build a robust agent that would be keep delivering reliable electricity everywhere and also operate the grid safely when an agent takes unknown adversarial actions at regular intervals.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;DETAILS&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset contained episodes spanning different adversarial actions taken by the agent. The agent could terminate any 1 of the 10 possible power lines (some of them being high voltage lines). Our agent was to be evaluated on how long it can provide reliable power to consumers without causing blackout. Once a blackout occurs, it is game over for that episode. The operation cost to be minimized included powerline losses, redispatch cost and blackout cost.&lt;/p&gt;
&lt;p&gt;Every substation in the competition grid had a &amp;ldquo;double busbar layout&amp;rdquo;. Hence, there was a choice of bus for making a connection from one of the bus to a grid object. Due to this and the medium sized grid used, the action space was of the order of 10^5 with the combinatorial action space reaching infinity.&lt;/p&gt;
&lt;p&gt;Our approach was derived from a previous &lt;a href=&#34;https://github.com/ZM-Learn/L2RPN_WCCI_a_Solution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;public solution&lt;/a&gt; for this competition. THe idea was to reduce the action space and train 2 A3C models with different training parameters.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:#5DADE2;font-style:bold;font-size:120%&#34;&gt;IMPACT&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We managed to improve upon the baseline set and managed to achieve a score of 10.84, which was the 21st highest score on the leaderboard, gaining us 28th rank.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

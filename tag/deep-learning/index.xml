<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Prasang Gupta</title>
    <link>https://prasang-gupta.github.io/tag/deep-learning/</link>
      <atom:link href="https://prasang-gupta.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Prasang Gupta</copyright><lastBuildDate>Mon, 01 Aug 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://prasang-gupta.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning</title>
      <link>https://prasang-gupta.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Contract Lifecycle Management (CLM)</title>
      <link>https://prasang-gupta.github.io/project/contractlifecyclemgmnt/</link>
      <pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/contractlifecyclemgmnt/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;PROBLEM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The client wanted to switch from legacy contract management tool to a new vendor. However, they were not confident of the metadata they had for the contracts and wanted help in getting all the major information extracted from the documents. They also wanted hierarchical linkages for most of the contracts for better organisation in the new tool.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;SOLUTION&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We solved the first problem with a myriad of different tools. These included advanced NLP models for entity extraction based on transformers, tree based ML models for certain classifications and logic-driven string search models.&lt;/p&gt;
&lt;p&gt;Common fields that are present in almost every document were extracted using a trained NER model linked with an active learning pipeline. These models were built using a modified version of &lt;a href=&#34;https://github.com/princeton-nlp/PURE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PURE&lt;/a&gt;, an open source offering by Princeton NLP. The active learning pipeline was built using LabelStudio as a frontend for manual annotations to get initial training data and then subsequently, for getting labels for the most uncertain samples.&lt;/p&gt;
&lt;p&gt;Fields which had only a few possible values like &amp;ldquo;Agreement Type&amp;rdquo;, were trained using a simple Random Forest classifier. The training data was obhtained using a mix of string extractions from the filenames and the existing contract metadata provided by the client. This, combined with the NER model resulted in boosted accuracies for this field.&lt;/p&gt;
&lt;p&gt;We built logic-based string search models for those fields which were not abundantly present and were more situational. These included fields like &amp;ldquo;Force Majeure&amp;rdquo; and all fields associated with this, &amp;ldquo;Termination Payments&amp;rdquo;, etc. This helped us extract information for these fields for the majority of the documents they were present in with little overhead.&lt;/p&gt;
&lt;p&gt;All of these extractions were provided to the client and after spot-checking and QA, were used in the construction of hierarchical linkages. Strong linkages were built by extracting the reference phrase within contracts that contains the details regarding the parent or the child contract. These were then translated back to the database we had. Weak linkages were also built based off of logic to combine documents under each MSA umbrella wherever the extraction of the reference phrase wasn&amp;rsquo;t possible.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The extractions provided to the client passed the QA test with about 90% accuracy. Also, we managed to provide hierchical linkages for more than 75% of the documents provided to us that could be linked.&lt;/p&gt;
&lt;p&gt;This has historically been a manual-only job with several hours of manpower invested in reading through the huge contracts and extracting information. We accelerated this process by appending this with models wherever possible. This reduced the load on the manual annotators and we were able to complete this in a fraction of the time that would&amp;rsquo;ve been invested in an all-manual project &lt;span style=&#34;color:#5DADE2;font-style:bold&#34;&gt;saving roughly 30,000 hours and $750,000 for the client&lt;/span&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zero-Shot Deepfake Audio Generation</title>
      <link>https://prasang-gupta.github.io/project/deepfakeaudio/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/deepfakeaudio/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to create a demo for generating speech from text in the voice of the user given the least amount of sample possible.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As the aim of this study was to create a demo, we chose to go ahead with Zero-Shot model as it takes practically no time to train and can start giving good results even with very little amount of sample data. The tradeoff with accuracy was accepted in favour of time. We tested different models, the most popular being YourTTS, SCGlowTTS and SV2TTS. On the basis of qualitative assessment on the representative test dataset, which included both male and female audio samples in 3 different accents (American, British and Indian), YourTTS model was selected as the clear winner.&lt;/p&gt;
&lt;p&gt;After finalising the model, this was tested further for robustness and a minimum sample audio time was estimated based off of these tests and it came out to be 1 minute for getting decent results in most cases. We then deployed this solution on an internal hosting site and created an interactive demo using our developed model as the backend. A representative set of 10 small sentences were obtained which contain as many different phonetics as possible and then a feature was added to record the user speak these sentences.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After running these samples through the model, which takes about a minute, the user would be able to hear their deepfake speaking 5 different sentences that were not a part of the representative set in their voice. This rounded off a user-friendly demo that can be used for GTM strategies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intelligent Systematic Investment Agent: an ensemble of deep learning and evolutionary strategies</title>
      <link>https://prasang-gupta.github.io/publication/rlisia/</link>
      <pubDate>Thu, 24 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/publication/rlisia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Intelligent Systematic Investment Agent</title>
      <link>https://prasang-gupta.github.io/project/rlisia/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/rlisia/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to explore Evolutionary strategies as an alternative to the classicaly used Reinforcement learning techniques to solve the ETF trading investment problem.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Historically, autonomous agents for trading and investment are generally built using one of the many reinforcement learning techniques. These include the SOTA Actor-critic algorithms as well as simpler DQN based algorithms. The problem with these is that an environment needs to be setup for training them and the learning curve for these algorithms is a bit steep for someone inexperienced in the field of Reinforcement Learning.&lt;/p&gt;
&lt;p&gt;We demonstrated that using Evolutionary strategies to solve episodic problems (which is getting daily purchase actions for a month in the current problem) can be a much easier and robust method. We also coupled this with a simple Neural network model to learn the patterns that are being revealed in the outputs of the ES runs. This broke down the problem into 2 mutually exclusive parts, which is much easier to understand and code.&lt;/p&gt;
&lt;p&gt;We compared both qualitative and quantitative metrics for our ensemble algorithm we call &lt;code&gt;GADLE&lt;/code&gt; with 2 traditional RL-based solutions, Actor-critic and DQN. Qualitatively, we compared the ease of writing of code and ease of understanding. Quantitatively, we ran different experiments centered around performance, sensitivity and consistency of the solutions.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proposed GADLE algorithm performs at par with the Actor-critic algorithm. However, it crushes the competition in terms of consistency and sensitivity. To put things in perspective, following are the tables summarising results of the sensitivity and the consistency experiments.&lt;/p&gt;














&lt;figure  id=&#34;figure-sensitivity-comparison-between-gadle-and-other-algorithms&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Sensitivity comparison between GADLE and other algorithms&#34; srcset=&#34;
               /project/rlisia/sensitivity_hu3a1eeebc4dcc2d597374b48d46a515c0_64132_22fa2060a5d243edc1acb6bcf95e40b6.png 400w,
               /project/rlisia/sensitivity_hu3a1eeebc4dcc2d597374b48d46a515c0_64132_b698d20a98ed40fe2b0c6afb9603e12a.png 760w,
               /project/rlisia/sensitivity_hu3a1eeebc4dcc2d597374b48d46a515c0_64132_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/rlisia/sensitivity_hu3a1eeebc4dcc2d597374b48d46a515c0_64132_22fa2060a5d243edc1acb6bcf95e40b6.png&#34;
               width=&#34;760&#34;
               height=&#34;253&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sensitivity comparison between GADLE and other algorithms
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-consistency-comparison-between-gadle-and-other-algorithms&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Consistency comparison between GADLE and other algorithms&#34; srcset=&#34;
               /project/rlisia/consistency_hu1ca179bcb047f7a1fbab8caa7874e21b_35741_4e41d84c91b3ce3cee113f3b57a93fe0.png 400w,
               /project/rlisia/consistency_hu1ca179bcb047f7a1fbab8caa7874e21b_35741_465f442911f10bfc7200508a17a3f094.png 760w,
               /project/rlisia/consistency_hu1ca179bcb047f7a1fbab8caa7874e21b_35741_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/rlisia/consistency_hu1ca179bcb047f7a1fbab8caa7874e21b_35741_4e41d84c91b3ce3cee113f3b57a93fe0.png&#34;
               width=&#34;760&#34;
               height=&#34;190&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Consistency comparison between GADLE and other algorithms
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Model Parallelism for Inference at edge</title>
      <link>https://prasang-gupta.github.io/project/modelparallelism/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/modelparallelism/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to create a solution that can be used to train a model that can be split into multiple pieces and can be run in parallel on multiple separate devices.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;














&lt;figure  id=&#34;figure-solution-architecture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Solution Architecture&#34; srcset=&#34;
               /project/modelparallelism/architecture_hub5f90f1de448dc6defbc2ca2b31e2b14_73424_215c88f36fa3407aa164683e7c3e7ca6.png 400w,
               /project/modelparallelism/architecture_hub5f90f1de448dc6defbc2ca2b31e2b14_73424_37006373126bde36e2cc0a8a4e097abb.png 760w,
               /project/modelparallelism/architecture_hub5f90f1de448dc6defbc2ca2b31e2b14_73424_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/modelparallelism/architecture_hub5f90f1de448dc6defbc2ca2b31e2b14_73424_215c88f36fa3407aa164683e7c3e7ca6.png&#34;
               width=&#34;760&#34;
               height=&#34;120&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Solution Architecture
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The architecture used was the recently published DeCNN (Decoupled CNN) network. The authors had used GConv with Channel Shuffle and added other network and OS level modifications to improve the performance of the decoupled architecture. For initial testing, we only focused on Scheme 1 and ignored other optimisations to establish a baseline.&lt;/p&gt;
&lt;p&gt;We used the tiny imagenet dataset and Resnet-34 model for testing and comparing the standard CNN results with DeCNN. We compared based on the performance (accuracy metrics) and the time taken to run inference on a fixed batch of data. It was found that there was a performance decrease by using DeCNN as some of the information is lost while doing channel shuffling. To counter that, we used 1.5x kernels as in the standard CNN to keep the performance comparable. Even while using just the Scheme 1 mentioned above, we got about 15% bump up in inference speeds with just 2 devices. We expect that this gap would increase with larger models and with more number of parallel devices.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution developed can be used to train models specifically for running on smaller edge devices. This is especially helpful as no single device can hold the full model in memory at the edge due to size and compute limitations and this would help in running bigger and better models at the edge with no added requirement of a heavy compute engine deployment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concept Drift Detection with Chatbots</title>
      <link>https://prasang-gupta.github.io/project/conceptdrift/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/conceptdrift/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to create a drift detection toolkit that can be easily used to detect drift in any type of data (image, audio or text) using a multitude of different drift detection methods to suit every problem under the sun. We also tested this toolkit&amp;rsquo;s ease of use by several case studies of different mock data types and also by integrating this with a chatbot built using RASA architecture to generate a novel drift-aware monitored chatbot.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The drift detection toolkit was built using several different drift detection methods like :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Drift Detection Type&lt;/th&gt;
&lt;th&gt;Drift Detection Methods&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Data distribution based methods&lt;/td&gt;
&lt;td&gt;- Kolmogorov-Smirnov (KS) test&lt;br&gt;- Maximum Mean Discrepancy (MMD) test&lt;br&gt;- Least-Squares Density Difference (LSDD) test&lt;br&gt;- KMeans and Chi Square Test&lt;br&gt;- Equal Intensity KMeans (EIKMeans) and Chi Square Test&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Drift magnitude based methods&lt;/td&gt;
&lt;td&gt;- Relative drift using Jensen–Shannon (JS) Divergence&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Uncertainty based methods&lt;/td&gt;
&lt;td&gt;- Uncertainty Classifier&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Error rate based methods&lt;/td&gt;
&lt;td&gt;- Fisher’s Test&lt;br&gt;- Statistical Test of Equal Proportions (STEPD)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We implemented these methods and verified that these methods are functioning properly using curated open-source datasets. After verifying all these methods for different types of data (text, audio and images), we implemented an integrated architecture with a chatbot built using RASA framework.&lt;/p&gt;














&lt;figure  id=&#34;figure-rasa-process-flow&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;RASA process flow&#34; srcset=&#34;
               /project/conceptdrift/architecture_hu33e64db526110706ea522e7294c7828c_88356_2a4979ed24c2fd3cbcdcf52825f299e9.png 400w,
               /project/conceptdrift/architecture_hu33e64db526110706ea522e7294c7828c_88356_478f1884e70c51c3c2bfed12ad677330.png 760w,
               /project/conceptdrift/architecture_hu33e64db526110706ea522e7294c7828c_88356_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/conceptdrift/architecture_hu33e64db526110706ea522e7294c7828c_88356_2a4979ed24c2fd3cbcdcf52825f299e9.png&#34;
               width=&#34;760&#34;
               height=&#34;440&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      RASA process flow
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The process flow diagram shows the working of the integrated system. We implemented several novel methods that ensured our solution remained as general as possible and it does not hamper the whole process in any way whatsoever. Additionally, in case drift is detected, we also made a training pipeline that would incorporate the changes in the model weights without having any model downtime.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution developed can be implemented with most of the chatbots that are currently in production. Implementing our system would ensure that the model does not lose intent quickly, and if it does, then proper notifications are being provided to the user based on the drift detection systems in place. It would also provide the developer with all the data that is needed to troubleshoot any issues and if needed, retrain the model to counter the drift without experiencing any downtimes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spatial  AI</title>
      <link>https://prasang-gupta.github.io/project/spatialai/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/spatialai/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this study was to test out some of the features offered by Worlds.io and create a generalised digital twin solution that can be leveraged in future projects. This would include generating a digital replication of their current environment and answer some of the questions based off of that with some additional analysis to allow clients to make better oeprational and strategic decisions.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;animation.gif&#34; alt=&#34;Solution Architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;We took the live footage provided by the vendor in New York City&amp;rsquo;s Bryant Park as a case study. A YOLO model was put on the raw camera footage to capture some common objects like people, bins, animals (pets) etc. This was then converted into a streaming data format and sent to cloud with approximate latitude and longitude values as provided by the vendor based on the camera&amp;rsquo;s location and field of view. These lat-lon values were then converted into cartesian coordinates, effectively treating Bryant park in a 2D plane for getting the data ready for digital twin studies. We also created a few zones and mapped the coordinates accordingly.&lt;/p&gt;
&lt;p&gt;The conversion to cartesian coordinates helped us to map the pedestrian movement patterns in the field of view. Additionally, we also extracted other information to answer questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How many people are passing through our zones in the park?&lt;/li&gt;
&lt;li&gt;How long are people generally in the park for?&lt;/li&gt;
&lt;li&gt;How many people are sitting in the park?&lt;/li&gt;
&lt;li&gt;How many people are walking babies/strollers?&lt;/li&gt;
&lt;li&gt;How many tourists/suitcases ?&lt;/li&gt;
&lt;li&gt;Are people riding bikes in the park?&lt;/li&gt;
&lt;li&gt;How many deliveries with carts are made?&lt;/li&gt;
&lt;li&gt;How many people are walking pets in the park?&lt;/li&gt;
&lt;li&gt;How many people per hour, walking speed, which way are they going,&lt;/li&gt;
&lt;li&gt;What % of people are stopping?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We were able to run analytics on the video feed and the derived variables. Also, we were able to generate a digital twin using the converted coordinates. This helped us build capability for any future projects that might entail the use of these technologies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HTML UI element extraction</title>
      <link>https://prasang-gupta.github.io/project/websiteuidetection/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/websiteuidetection/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$2^{nd}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this hackathon was to localise and identify several different HTML UI elements in hand-drawn wireframe drawings of websites as well as for screenshots of real websites.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset provided for the hackathon contained about 3200 wireframe drawings of websites. The goal was to identify the different HTML UI elemnents present in the image, such as &amp;ldquo;Text Box&amp;rdquo;, &amp;ldquo;Button&amp;rdquo;, &amp;ldquo;Image&amp;rdquo;, etc. Hence, it boiled down to an object detection problem. It also included another dataset containing screenshots of real websites. The problem remained the same for both the datasets.&lt;/p&gt;
&lt;p&gt;We used several different Object Detection techniques and decided on using the just released SOTA model YOLOv5. We tried different flavours of YOLOv5 and since inference time was not a bar, we went ahead with the XL version of the same to boost performance. We also used pre-trained weights and performed a LR scheduler study to boost the scores even further.&lt;/p&gt;
&lt;p&gt;We also observed that our model was giving out good predictions for the common elements with high confidences, but was not giving outputs for the more uncommon elements. Hence, we performed a study to change the confidence cutoff levels to optimise it for the use case and adjust it according to the distribution in the data. This allowed us to achieve near-perfect performance level of 0.95 F1 score on the validation set.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution built was performing really good on unseen test images managing an mAP value of 0.82. This model was later swapped with the last year&amp;rsquo;s model in the already developed pipeline to allow rapid prototyping of websites and dashboards.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Twitter Bias Hackathon</title>
      <link>https://prasang-gupta.github.io/project/twitterbiashack/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/twitterbiashack/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$9^{th}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of the hackathon was to higlight bias in Twitter&amp;rsquo;s Saliency model.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Twitter&amp;rsquo;s Saliency model is a model that is used to crop over-sized images to fit on the screen representing a thumbnail preview. The saliency model is responsible for selecting the most &amp;ldquo;salient&amp;rdquo; part of the image and that part is consequently kept in the thumbnail and the area surroinding it is cropped off.&lt;/p&gt;
&lt;p&gt;We decided to chose, recently concluded at the time, Tokyo Olympics 2022 as a case study for highlighting bias. We downloaded around 5000 images across 10 different sports and ran the saliency model on top of them. We also passed these images through an object detection model to identify images with and without athletes.&lt;/p&gt;
&lt;p&gt;We ran several studies, the main being classifying the images and the crops as something that was biased or not-biased. We also ran a sensitivity analysis and also tried observing changes between original coloured images and an image with a filter put on it (sepia and grayscale).&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We found out that the model, in some cases, was biased towards text present in the image (which was mostly Tokyo 2020). We also found in several cases, where the main focus of the image, the athletes were not the ones selected as the most salient in the image, instead, the saliency model was predicting either someone from the audience or billboards / advertisements as the most salient. This report was submitted to Twitter and we got a thanks from the Twitter team for highlighting this bias in their model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Demand Prediction with Competition Analysis</title>
      <link>https://prasang-gupta.github.io/project/demandprediction/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/demandprediction/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this study was to predict consumer demand using data from competition and infusing it with other datasets that influence demand like mobility and demographic variables. The other goal was to correct a proprietary mobility dataset used by PwC to incorporate for mobility changes brought on by COVID restrictions.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proprietary dataset mentioned had several flaws including data inconsistencies over months and years and noisy data which was not helping in developing stable models. It also didn&amp;rsquo;t account for any socio-economic variables based on the demographics of the region. To correct this, the proprietary dataset was merged with demographics variables based on the location and some feature engineering was done to get the mobility in radii of 1km to 5km based on the store location. The final sales value was kept as the dependent field for the model.&lt;/p&gt;
&lt;p&gt;Multiple model architectures were tried to model this information to correct the mobility, including ML and DL models. Finally, a voting ensemble method was selected which was modelling the corrected demand with an accuracy of about 70%.&lt;/p&gt;
&lt;p&gt;To bump up the numbers further, an aerial snapshot of the region around the score was obtained from Google Satellite API at a tuned zoom level and a segmentation exercise was performed on the image to extract the type of region around the store. This was expected to bring another dimension of immediate demographics in the dataset. This was done by extracting the coverage area of different colors based on pixel masks :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grey (signifying roads, parking lots, streets, etc)&lt;/li&gt;
&lt;li&gt;Green (signifying agriculture, parks, vegetation, etc)&lt;/li&gt;
&lt;li&gt;White (signifying roof-tops, buildings, etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The final model with all the variables included (engineered visits from proprietary data, Google mobility data, demographics data and derived immediate demographics of the store from satellite images) achieved a test accuracy of 75%. This improved the quality of the mobility data and impacted tens of projects that utilised this mobility dataset helping in building much more accurate and robust models for client deliveries.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HTML Atomic UI Elements Extraction from Hand-Drawn Website Images using Mask-RCNN and novel Multi-Pass Inference Technique</title>
      <link>https://prasang-gupta.github.io/publication/clef2020/</link>
      <pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/publication/clef2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>HTML UI element extraction</title>
      <link>https://prasang-gupta.github.io/project/wireframeuidetection/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/wireframeuidetection/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$3^{rd}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this hackathon was to localise and identify several different HTML UI elements in hand-drawn wireframe drawings of websites.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset provided for the hackathon contained about 3000 wireframe drawings of websites. The goal was to identify the different HTML UI elemnents present in the image, such as &amp;ldquo;Text Box&amp;rdquo;, &amp;ldquo;Button&amp;rdquo;, &amp;ldquo;Image&amp;rdquo;, etc. Hence, it boiled down to an object detection problem.&lt;/p&gt;
&lt;p&gt;We tried using several Object Detection algorithms like YOLO, R-CNN and Mask-RCNN and decided on Mask-RCNN as it was providing us with the best results. However, one thing we observed in our outputs was that our Precision scores were good, but the model was lacking in Recall bringing the whole F1 down. To solve this problem, we came up with a novel technique &amp;ldquo;Multi-Pass Inference&amp;rdquo; that booosted our recall scores.&lt;/p&gt;
&lt;p&gt;The technique involves running the image through the model multiple times, each time taking note of the objects that are already detected and removing them for subsequent passes. This forced the model to predict more instances of the elements in the image. We smarlty combined the objects detected in multiple passes to overall boost the recall score of our model helping us to take a podium spot in the leaderboard.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution built was performing really good on unseen test images managing an mAP (IoU &amp;gt; 0.5) score of 64.12. This solution was later implemented into a pipeline to allow rapid prototyping of websites and dashboards.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active Learning</title>
      <link>https://prasang-gupta.github.io/project/activelearning/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/activelearning/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of the study was to build out a pipeline for active learning that other projects with scarce labelled data will leverage for model building. It also included trying the effectiveness of active learning on a standard dataset and evaluate different techniques.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset chosen for this study was CIFAR10. This dataset contains 60,000 32x32 images with 10 unique classes. We used a maximum of 10,000 images at each time for training purposes. This study&amp;rsquo;s focus was to compare the performance of active and passive learning using same number of labelled samples.&lt;/p&gt;
&lt;p&gt;The passive learning model would be trained on a uniform distribution of the dataset size, however, the active learning model would be initialised with half the training size and then would be trained for the full size by sequential queries. We tried 3 queries in this study : uncertainty, margin sampling and entropy sampling. We achieved better performance with active learning for all the sizes, however, the improvement in performance varied from as low as 1% to a sizable 23%. The details for all the experiments and the queries can be seen in the slides.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The pipeline developed from this project was consecutively used in multiple client projects to improve the modelling capabilities and thrive wherever manually labelling the full dataset was not an option.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implementation of live models on edge</title>
      <link>https://prasang-gupta.github.io/project/edgelivemodels/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/edgelivemodels/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was two fold. The first aim was to implement an action recognition model and the second was to modify it to run on an edge device. For this, we chose the Pre-trained Temporal Relation Network Model. The dataset chosen for this was the 20BN-something-something Dataset V2. This dataset has over 100 classes of different object-human or object-object interactions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;jetson.png&#34; alt=&#34;Jetson TX2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The model was first implemented on a laptop with the webcam and then later extended onto the edge device, Jetson TX2. Prior to this, the TX2 was flashed and proper libraries were built from source to enable it to use its full potential (CUDA cores for rendering). We were successfully able to implement this on the board and were getting very respectable frame rates, somewhere around 10 fps. The performance of this model for several different scenarios can be seen in the videos link and a little detail about the implementation can be found in the slides.&lt;/p&gt;
&lt;p&gt;Apart from this, we also implemented simple object detection models on Raspberry Pi on which we were getting around 1-2 fps.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These implementations paved the way for future projects that involved hosting models on smaller edge devices. These capabilities were built for the first time within the team.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Damaged Car Parts Segmentation for auto claims</title>
      <link>https://prasang-gupta.github.io/project/carpartsegmentation/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/carpartsegmentation/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;PROBLEM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The client wanted to reduce the time spent by their employees in looking through several different photographs submitted for insurance claims clearing and ascertain damaged parts of the vehicle with the extent of damage.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;SOLUTION&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We solved the problem by training a semantic segmentation model used for ascertainining the different kinds of damage that were present in the photograph of a vehicle (like the figure attached). To ensure the correctness of the model, we also employed an explainable AI technique, LIME, which returned the parts of the image it is looking at when coming to a decision about the damage of a particular type.&lt;/p&gt;
&lt;p&gt;A few classification models were also trained to fetch images which were visually similar with the current image. The different variants were the similar damage model, where the model would return the top images which have visually similar damage to the current image. The other variant was the similar non-damage model, in which the model would return the top images of similar vehicles of previously processed claims.&lt;/p&gt;
&lt;p&gt;To top it all off, an automated report generation tool was coupled with the model (using FPDF), which returned a formatted PDF report having detailed information regarding the damage and the claims (a sample report attached).&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The segmentation model ensured that the client team spent lesser time on figuring out the damage and more time providing personalised support to the consumers. The classification models helped the client team to look at some previous claims to decide the outcome for the current claim in a more informed and consistent manner. This improved the overall reputation of the firm in disbursing out claims. Also, the automated report could be handed over to the consumers directly for a much more transparent view into the claims processing.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Prasang Gupta</title>
    <link>https://prasang-gupta.github.io/tag/deep-learning/</link>
      <atom:link href="https://prasang-gupta.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Prasang Gupta</copyright><lastBuildDate>Tue, 01 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://prasang-gupta.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Deep Learning</title>
      <link>https://prasang-gupta.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Active Learning</title>
      <link>https://prasang-gupta.github.io/project/activelearning/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/activelearning/</guid>
      <description>&lt;p&gt;The aim of the study was to try the effectiveness of active learning for cases where labelled data is scarce. The dataset chosen for this study was CIFAR10. This dataset contains 60,000 images of size 32x32 with 10 classes. The images are single label and are mutually exclusive from each other. We have used a maximum of 10,000 images at each time for trainig purposes.&lt;/p&gt;
&lt;p&gt;This study&amp;rsquo;s focus was to compare the performance of active and passive learning on the same dataset size. The passive learning model would be trained on a uniform distribution of the dataset size, however, the active learning model would be initialised with half the training size and then would be trained for the full size by sequential queries. We have tried 3 queries in this study : uncertainty, margin sampling and entropy sampling. We achieved better performance with active learning for all the sizes, however, the difference between the performance of the two varied depending on the total size of the dataset going from as low as 1% to a sizable 23%. The details for all the experiments and the queries can be seen in the slides.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Demand Prediction with Competition Analysis</title>
      <link>https://prasang-gupta.github.io/project/demandprediction/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/demandprediction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Trainings Repository</title>
      <link>https://prasang-gupta.github.io/project/lndrepo/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/lndrepo/</guid>
      <description>&lt;p&gt;The aim of the study was to try the effectiveness of active learning for cases where labelled data is scarce. The dataset chosen for this study was CIFAR10. This dataset contains 60,000 images of size 32x32 with 10 classes. The images are single label and are mutually exclusive from each other. We have used a maximum of 10,000 images at each time for trainig purposes.&lt;/p&gt;
&lt;p&gt;This study&amp;rsquo;s focus was to compare the performance of active and passive learning on the same dataset size. The passive learning model would be trained on a uniform distribution of the dataset size, however, the active learning model would be initialised with half the training size and then would be trained for the full size by sequential queries. We have tried 3 queries in this study : uncertainty, margin sampling and entropy sampling. We achieved better performance with active learning for all the sizes, however, the difference between the performance of the two varied depending on the total size of the dataset going from as low as 1% to a sizable 23%. The details for all the experiments and the queries can be seen in the slides.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implementation of Live models on Edge</title>
      <link>https://prasang-gupta.github.io/project/edgelivemodels/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/edgelivemodels/</guid>
      <description>&lt;p&gt;The aim of this project was two fold. The first aim was to get the action recognition model running and the second was to implement the same on an edge device. For this, we chose the Pre-trained Temporal Relation Network Model from &lt;a href=&#34;https://github.com/zhoubolei/TRN-pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The dataset chosen for this was the &lt;a href=&#34;https://20bn.com/datasets/something-something/v2#download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;20BN-something-something Dataset V2&lt;/a&gt;. This dataset has over 100 classes of different object-human or object-object interactions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;jetson.png&#34; alt=&#34;Jetson TX2&#34;&gt;&lt;/p&gt;
&lt;p&gt;The model was first implemented on a laptop with the webcam and then later extended onto the edge device, Jetson TX2. Prior to this, the TX2 was flashed and proper libraries were built from source to enable it to use its full potential (CUDA cores for rendering). We were successfully able to implement this on the board and were getting very respectable frame rates, somewhere around 10 fps. The performance of this model for several different scenarios can be seen in the videos link and a little detail about the implementation can be found in the slides.&lt;/p&gt;
&lt;p&gt;Apart from this, we also implemented simple object detection models on Raspberry Pi on which we were getting around 1-2 fps.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Damaged Car Parts Segmentation for auto claims</title>
      <link>https://prasang-gupta.github.io/project/carpartsegmentation/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/carpartsegmentation/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;em&gt;CLIENT PROBLEM&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The client wanted to reduce the time spent by their employees on looking through several different photographs submitted for insurance claims clearing and ascertain damaged parts of the vehicle with the extent of damage.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;OUR SOLUTION&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We solved the problem by training a semantic segmentation model used for ascertainining the different kinds of damage that were present in the photograph of a vehicle (like the figure attached). To ensure the correctness of the model, we also employed an explainable AI technique, LIME, which returns the parts of the image it is looking at when coming to a decision about the damage.&lt;/p&gt;
&lt;p&gt;A few classification models were also trained to fetch images which were visually similar with the current image. The different variants were the similar damage model, where the model would return the top images which have visually similar damage to the current image. The other variant was the similar non-damage model, in which the model would return the top images of similar vehicles of previously processed claims.&lt;/p&gt;
&lt;p&gt;To top it all off, an automated report generation tool was coupled with the model (using FPDF), which returned a formatted PDF report having detailed information regarding the damage and the claims.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;VALUE GENERATED&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The segmentation model ensured that the client team spent lesser time on figuring out the damage and more time providing personalised support to the consumers. The classification models helped the client team to look at some previous claims to decide the outcome for the current claim in a more informed and consistent manner. This improved the overall reputation of the firm in disbursing out claims. Also, the automated report could be handed over to the consumers directly for a much more transparent view into the claims processing.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

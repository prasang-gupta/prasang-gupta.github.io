[{"authors":null,"categories":null,"content":"I am currently working in the Emerging Technologies team in PwC US Advisory. The team focuses primarily on providing innovative, cutting edge solutions to client problems leveraging AI, ML and IoT. In addition to client projects, we also build PoCs (Proof of Concepts) for different technologies which we think will bring value to clients and present them as potential replacements for outdated ways of working.\nBefore this, I did my Masters and Bachelors in Chemical Engineering from IIT Kanpur. I was interested in Simulation studies and did multiple projects in this domain including molecular dynamics simulations, simulating flows of particles and simulating burner profiles. All of this was built leveraging multiple open source packages and products along with self written optimised code.\nApart from this, I like board games, online strategy games and geeky tech stuff. I also like teaching and have been pursuing several opportunities to teach underprivileged students. I love keeping up with the latest trends in technology both in the hardware and the software domain.\nDownload my resumé.\n","date":1658102400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1658102400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am currently working in the Emerging Technologies team in PwC US Advisory. The team focuses primarily on providing innovative, cutting edge solutions to client problems leveraging AI, ML and IoT.","tags":null,"title":"Prasang Gupta","type":"authors"},{"authors":["Prasang Gupta","Shaz Hoda","Anand Rao"],"categories":null,"content":"","date":1658102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658102400,"objectID":"c0e6b75c8a0d1a501bdde39025f0fb71","permalink":"https://prasang-gupta.github.io/publication/rlisiapatent/","publishdate":"2022-07-18T00:00:00Z","relpermalink":"/publication/rlisiapatent/","section":"publication","summary":"Patent Pending (US Patent Office)","tags":[],"title":"Intelligent Systematic Investment Agent: an ensemble of deep learning and evolutionary strategies","type":"publication"},{"authors":["Joseph Voyles","Siddhesh Zanj","Prasang Gupta","Vishakha Bansal","Shantanu Dev"],"categories":null,"content":"","date":1657238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657238400,"objectID":"a1daee033f4e0eeabbe8922b394fd373","permalink":"https://prasang-gupta.github.io/publication/cdpatent/","publishdate":"2022-07-08T00:00:00Z","relpermalink":"/publication/cdpatent/","section":"publication","summary":"Patent Pending (US Patent Office)","tags":[],"title":"Integrated Monitoring System for Chatbots for Concept Drift Detection","type":"publication"},{"authors":["Prasang Gupta","Shaz Hoda","Anand Rao"],"categories":null,"content":"","date":1648080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648080000,"objectID":"7fd7e79d627f4b427e064bf95daeb42a","permalink":"https://prasang-gupta.github.io/publication/rlisia/","publishdate":"2022-03-24T00:00:00Z","relpermalink":"/publication/rlisia/","section":"publication","summary":"Under Review","tags":["Machine Learning","Investment","Reinforcement Learning","Deep Learning","Evolutionary Strategies","Genetic Algorithm","ETF Trading"],"title":"Intelligent Systematic Investment Agent: an ensemble of deep learning and evolutionary strategies","type":"publication"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this project was to create a demo for generating speech from text in the voice of the user given the least amount of sample possible.\nDETAILS\nAs the aim of this study was to create a demo, we chose to go ahead with Zero-Shot model as it takes practically no time to train and can start giving good results even with very little amount of sample data. The tradeoff with accuracy was accepted in favour of time. We tested different models, the most popular being YourTTS, SCGlowTTS and SV2TTS. On the basis of qualitative assessment on the representative test dataset, which included both male and female audio samples in 3 different accents (American, British and Indian), YourTTS model was selected as the clear winner.\nAfter finalising the model, this was tested further for robustness and a minimum sample audio time was estimated based off of these tests and it came out to be 1 minute for getting decent results in most cases. We then deployed this solution on an internal hosting site and created an interactive demo using our developed model as the backend. A representative set of 10 small sentences were obtained which contain as many different phonetics as possible and then a feature was added to record the user speak these sentences.\nAfter running these samples through the model, which takes about a minute, the user would be able to hear their deepfake speaking 5 different sentences that were not a part of the representative set in their voice.\n","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"23814bf7634f7bbd4b79b08a59e5c71b","permalink":"https://prasang-gupta.github.io/project/deepfakeaudio/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/project/deepfakeaudio/","section":"project","summary":"Created a web-hosted demo to generate audio from text in the voice of the user with a voice sample size of barely 1 minute using a zero-shot audio generation model","tags":["Zero-Shot","Deepfake","Deep Learning","Audio Generation","Text to Speech","Demo","Proof of Concept","ML-DL"],"title":"Zero-Shot Deepfake Generation - Audio","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this project was to create a solution that can be used to train a model that can be split into multiple pieces and can be run in parallel on multiple separate devices.\nDETAILS\nThe architecture used was the recently published DeCNN (Decoupled CNN) network. The authors had used GConv with Channel Shuffle and added other network and OS level modifications to improve the performance of the decoupled architecture. For initial testing, we only focused on Scheme 1 and ignored other optimisations to establish a baseline.\nWe used the tiny imagenet dataset and Resnet-34 model for testing and comparing the standard CNN results with DeCNN. We compared based on the performance (accuracy metrics) and the time taken to run inference on a fixed batch of data. It was found that there was a performance decrease by using DeCNN as some of the information is lost while doing channel shuffling. To counter that, we used 1.5x kernels as in the standard CNN to keep the performance comparable. Even while using just the Scheme 1 mentioned above, we got about 15% bump up in inference speeds with just 2 devices. We expect that this gap would increase with larger models and with more number of parallel devices.\nIMPACT\nThe solution developed can be used to train models specifically for running on smaller edge devices. This is especially helpful as no single device can hold the full model in memory at the edge due to size and compute limitations and this would help in running bigger and better models at the edge with no added requirement of a heavy compute engine deployment.\n","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"54d4bce5edd71c041881c68a3167f49d","permalink":"https://prasang-gupta.github.io/project/modelparallelism/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/project/modelparallelism/","section":"project","summary":"Implemented a solution that uses model parallelism for speeding up inference at the edge by distributing a single model across different devices","tags":["Model Parallelism","IoT","Edge","Deep Learning","Proof of Concept","ML-DL"],"title":"Model Parallelism for Inference at edge","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this project was to create a solution that would auto-create documentation given a code repository with docstrings. The main focus of this solution was be to allow generating quick documentation that can be hosted on Github pages for making it easier to re-use codebases because of a standardised documentation format.\nDETAILS\nWe created a documentation template and chose RTD (Read The Docs) as standard formatting because of its popularity among open source tools. We built 2 methods within the solution, first focused on generating and hosting documentation quickly (under 10 minutes) and the second focused on learning the nitty-gritties of how Sphinx works and providing much more room for modifications and personalisation. This system can also be used to generate documentation in PDF format using latex.\nIMPACT\nThe solution was used widely across different teams and it allowed easy re-use of code. It also cultivated a good culture of writing docstrings in functions and classes across the whole team.\n","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"3be3d863d45e35ef79bc6f13bd6c664a","permalink":"https://prasang-gupta.github.io/project/rtddocumentation/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/project/rtddocumentation/","section":"project","summary":"Built an easy-to-use and mostly automated solution using Sphinx to create quick documentation of code repositories in RTD format with support for hosting code documentation on Github Pages.","tags":["Read The Docs","Documentation","Toolkit","Automation","Firm Internal"],"title":"Read-The-Docs Documentation Templatisation","type":"project"},{"authors":["Prasang Gupta","Antoinette Young","Anand Rao"],"categories":null,"content":"","date":1634601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634601600,"objectID":"cdd85520e68c7d4e647d98f2a143c6c8","permalink":"https://prasang-gupta.github.io/publication/cargoloss/","publishdate":"2021-10-19T00:00:00Z","relpermalink":"/publication/cargoloss/","section":"publication","summary":"11th International Conference on Embedded Systems and Applications (EMSA 2022) - Sydney, Australia","tags":["Asset tracking","Logistics","Cargo loss","Cargo damage","Impact sensor","Accelerometer sensor","Low-cost solution","No code AEP (Application Enablement Platform)"],"title":"Investigating Cargo Loss in Logistics Systems using Low-Cost Impact Sensors","type":"publication"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this project was to explore Evolutionary strategies as an alternative to the classicaly used Reinforcement learning techniques to solve the ETF trading investment problem.\nDETAILS\nHistorically, autonomous agents for trading and investment are generally built using one of the many reinforcement learning techniques. These include the SOTA Actor-critic algorithms as well as simpler DQN based algorithms. The problem with these is that an environment needs to be setup for training them and the learning curve for these algorithms is a bit steep for someone inexperienced in the field of Reinforcement Learning.\nWe demonstrated that using Evolutionary strategies to solve episodic problems (which is getting daily purchase actions for a month in the current problem) can be a much easier and robust method. We also coupled this with a simple Neural network model to learn the patterns that are being revealed in the outputs of the ES runs. This broke down the problem into 2 mutually exclusive parts, which is much easier to understand and code.\nWe compared both qualitative and quantitative metrics for our ensemble algorithm we call GADLE with 2 traditional RL-based solutions, Actor-critic and DQN. Qualitatively, we compared the ease of writing of code and ease of understanding. Quantitatively, we ran different experiments centered around performance, sensitivity and consistency of the solutions.\nIMPACT\nThe proposed GADLE algorithm performs at par with the Actor-critic algorithm. However, it crushes the competition in terms of consistency and sensitivity. To put things in perspective, following are the tables summarising results of the sensitivity and the consistency experiments.\n","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"34fd66138200fb6df9c01d9c5d499d60","permalink":"https://prasang-gupta.github.io/project/rlisia/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/project/rlisia/","section":"project","summary":"Explored an ensemble of deep learning and evolutionary strategies as an RL alternative in the context of an investment problem","tags":["Investment","ETF Trading","Reinforcement Learning","Evolutionary Strategies","Genetic Algorithm","Deep Learning","Firm Internal","ML-DL"],"title":"Intelligent Systematic Investment Agent","type":"project"},{"authors":null,"categories":null,"content":"PROBLEM\nThe client wanted to switch from legacy contract management tool to a new vendor. However, they were not confident of the metadata they had for the contracts and wanted help in getting all the major information extracted from the documents. They also wanted hierarchical linkages for most of the contracts for better organisation in the new tool.\nSOLUTION\nWe solved the first problem with a myriad of different tools. These included advanced NLP models for entity extraction based on transformers, tree based ML models for certain classifications and logic-driven string search models.\nCommon fields that are present in almost every document were extracted using a trained NER model linked with an active learning pipeline. These models were built using a modified version of PURE, an open source offering by Princeton NLP. The active learning pipeline was built using LabelStudio as a frontend for manual annotations to get initial training data and then subsequently, for getting labels for the most uncertain samples.\nFields which had only a few possible values like \u0026ldquo;Agreement Type\u0026rdquo;, were trained using a simple Random Forest classifier. The training data was obhtained using a mix of string extractions from the filenames and the existing contract metadata provided by the client. This, combined with the NER model resulted in boosted accuracies for this field.\nWe built logic-based string search models for those fields which were not abundantly present and were more situational. These included fields like \u0026ldquo;Force Majeure\u0026rdquo; and all fields associated with this, \u0026ldquo;Termination Payments\u0026rdquo;, etc. This helped us extract information for these fields for the majority of the documents they were present in with little overhead.\nAll of these extractions were provided to the client and after spot-checking and QA, were used in the construction of hierarchical linkages. Strong linkages were built by extracting the reference phrase within contracts that contains the details regarding the parent or the child contract. These were then translated back to the database we had. Weak linkages were also built based off of logic to combine documents under each MSA umbrella wherever the extraction of the reference phrase wasn\u0026rsquo;t possible.\nIMPACT\nThe extractions provided to the client passed the QA test with about 90% accuracy. Also, we managed to provide hierchical linkages for more than 75% of the documents provided to us that could be linked.\nThis has historically been a manual-only job with several hours of manpower invested in reading through the huge contracts and extracting information. We accelerated this process by appending this with models wherever possible. This reduced the load on the manual annotators and we were able to complete this in a fraction of the time that would\u0026rsquo;ve been invested in an all-manual project saving roughly 30,000 hours and $750,000 for the client.\n","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"d34d2125c965f2f71d0e7147dcef6b0c","permalink":"https://prasang-gupta.github.io/project/contractlifecyclemgmnt/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/project/contractlifecyclemgmnt/","section":"project","summary":"Extracted information from legacy scanned copies of contracts and built hierarchical linkages as a part of contract lifecycle management for the client","tags":["Deep Learning","Document Understanding","Information Extraction","NLP","Active Learning","Hierarchy Construction","Client Delivery","ML-DL"],"title":"Contract Lifecycle Management (CLM)","type":"project"},{"authors":null,"categories":null,"content":"We achieved 9th rank in the hackathon.\nAIM\nThe aim of the hackathon was to higlight bias in Twitter\u0026rsquo;s Saliency model.\nDETAILS\nTwitter\u0026rsquo;s Saliency model is a model that is used to crop over-sized images to fit on the screen representing a thumbnail preview. The saliency model is responsible for selecting the most \u0026ldquo;salient\u0026rdquo; part of the image and that part is consequently kept in the thumbnail and the area surroinding it is cropped off.\nWe decided to chose, recently concluded at the time, Tokyo Olympics 2022 as a case study for highlighting bias. We downloaded around 5000 images across 10 different sports and ran the saliency model on top of them. We also passed these images through an object detection model to identify images with and without athletes.\nWe ran several studies, the main being classifying the images and the crops as something that was biased or not-biased. We also ran a sensitivity analysis and also tried observing changes between original coloured images and an image with a filter put on it (sepia and grayscale).\nIMPACT\nWe found out that the model, in some cases, was biased towards text present in the image (which was mostly Tokyo 2020). We also found in several cases, where the main focus of the image, the athletes were not the ones selected as the most salient in the image, instead, the saliency model was predicting either someone from the audience or billboards / advertisements as the most salient. This report was submitted to Twitter and we got a thanks from the Twitter team for highlighting this bias in their model.\n","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"741569038ce939b9e73c4aec106d4739","permalink":"https://prasang-gupta.github.io/project/twitterbiashack/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/project/twitterbiashack/","section":"project","summary":"Participated in the Twitter Bias Hackathon and highlighted bias instances in Twitter's Saliency model used to crop images building a case study with Tokyo Olympics images as example. Achieved 9th rank in the hackathon.","tags":["Bias","Responsible AI","Deep Learning","Tokyo Olympics 2022","Twitter","Saliency Model","Hackathon","ML-DL"],"title":"Twitter Bias Hackathon","type":"project"},{"authors":["Prasang Gupta","Vishakha Bansal"],"categories":null,"content":"","date":1627344000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627344000,"objectID":"6c906c2512f77845659c83dc0f1dc56f","permalink":"https://prasang-gupta.github.io/publication/clef2021/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/clef2021/","section":"publication","summary":"12th International Conference of the CLEF Association (CLEF 2021) - Bucharest, Romania","tags":["Website UI elements","UI element extraction","Image Processing","OpenCV","Object Detection","YOLOv5","Confidence Cutoff Variation"],"title":"UI element detection from wireframe drawings of websites","type":"publication"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this project was to create a drift detection toolkit that can be easily used to detect drift in any type of data (image, audio or text) using a multitude of different drift detection methods to suit every problem under the sun. We also tested this toolkit\u0026rsquo;s ease of use by several case studies of different mock data types and also by integrating this with a chatbot built using RASA architecture to generate a novel drift-aware monitored chatbot.\nDETAILS\nThe drift detection toolkit was built using several different drift detection methods like :\nDrift Detection Type Drift Detection Methods Data distribution based methods - Kolmogorov-Smirnov (KS) test\n- Maximum Mean Discrepancy (MMD) test\n- Least-Squares Density Difference (LSDD) test\n- KMeans and Chi Square Test\n- Equal Intensity KMeans (EIKMeans) and Chi Square Test Drift magnitude based methods - Relative drift using Jensen–Shannon (JS) Divergence Uncertainty based methods - Uncertainty Classifier Error rate based methods - Fisher’s Test\n- Statistical Test of Equal Proportions (STEPD) We implemented these methods and verified that these methods are functioning properly using curated open-source datasets. After verifying all these methods for different types of data (text, audio and images), we implemented an integrated architecture with a chatbot built using RASA framework.\nThe process flow diagram shows the working of the integrated system. We implemented several novel methods that ensured our solution remained as general as possible and it does not hamper the whole process in any way whatsoever. Additionally, in case drift is detected, we also made a training pipeline that would incorporate the changes in the model weights without having any model downtime.\nIMPACT\nThe solution developed can be implemented with most of the chatbots that are currently in production. Implementing our system would ensure that the model does not lose intent quickly, and if it does, then proper notifications are being provided to the user based on the drift detection systems in place. It would also provide the developer with all the data that is needed to troubleshoot any issues and if needed, retrain the model to counter the drift without experiencing any downtimes.\n","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"05eb04e0bf2913f1704f64ab9bed2672","permalink":"https://prasang-gupta.github.io/project/conceptdrift/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/project/conceptdrift/","section":"project","summary":"Built a scikit-learn-like toolkit with capabilities to detect concept drift in image, text or audio data using an array of drift detection methods for monitoring and extending the life of production models","tags":["Concept Drift","Concept Evolution","Toolkit","Data Drift","Model Drift","Deep Learning","Firm Internal","ML-DL"],"title":"Concept Drift Detection with Chatbots","type":"project"},{"authors":null,"categories":null,"content":"We achieved 3rd rank in the hackathon.\nAIM\nThe aim of this hackathon was to localise and identify several different HTML UI elements in hand-drawn wireframe drawings of websites as well as for screenshots of real websites.\nDETAILS\nThe dataset provided for the hackathon contained about 3200 wireframe drawings of websites. The goal was to identify the different HTML UI elemnents present in the image, such as \u0026ldquo;Text Box\u0026rdquo;, \u0026ldquo;Button\u0026rdquo;, \u0026ldquo;Image\u0026rdquo;, etc. Hence, it boiled down to an object detection problem. It also included another dataset containing screenshots of real websites. The problem remained the same for both the datasets.\nWe used several different Object Detection techniques and decided on using the just released SOTA model YOLOv5. We tried different flavours of YOLOv5 and since inference time was not a bar, we went ahead with the XL version of the same to boost performance. We also used pre-trained weights and performed a LR scheduler study to boost the scores even further.\nWe also observed that our model was giving out good predictions for the common elements with high confidences, but was not giving outputs for the more uncommon elements. Hence, we performed a study to change the confidence cutoff levels to optimise it for the use case and adjust it according to the distribution in the data. This allowed us to achieve near-perfect performance level of 0.95 F1 score on the validation set.\nIMPACT\nThe solution built was performing really good on unseen test images managing an mAP value of 0.82. This model was later swapped with the last year\u0026rsquo;s model in the already developed pipeline to allow rapid prototyping of websites and dashboards.\n","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"d2ba98a60e5dcca345bddb593e0eb77e","permalink":"https://prasang-gupta.github.io/project/websiteuidetection/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/project/websiteuidetection/","section":"project","summary":"Extracted HTML UI elements from wireframe drawings of websites and website screenshots using advanced image pre-processing and confidence cutoff variation. Achieved 2nd rank in the hackathon.","tags":["HTML","UI","Image Processing","Deep Learning","OpenCV","YOLOv5","AICrowd","Hackathon","ML-DL"],"title":"HTML UI element extraction","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this study was to test out some of the features offered by Worlds.io and create a generalised digital twin solution that can be leveraged in future projects. This would include generating a digital replication of their current environment and answer some of the questions based off of that with some additional analysis to allow clients to make better oeprational and strategic decisions.\nDETAILS\nWe took the live footage provided by the vendor in New York City\u0026rsquo;s Bryant Park as a case study. A YOLO model was put on the raw camera footage to capture some common objects like people, bins, animals (pets) etc. This was then converted into a streaming data format and sent to cloud with approximate latitude and longitude values as provided by the vendor based on the camera\u0026rsquo;s location and field of view. These lat-lon values were then converted into cartesian coordinates, effectively treating Bryant park in a 2D plane for getting the data ready for digital twin studies. We also created a few zones and mapped the coordinates accordingly.\nThe conversion to cartesian coordinates helped us to map the pedestrian movement patterns in the field of view. Additionally, we also extracted other information to answer questions like:\nHow many people are passing through our zones in the park? How long are people generally in the park for? How many people are sitting in the park? How many people are walking babies/strollers? How many tourists/suitcases ? Are people riding bikes in the park? How many deliveries with carts are made? How many people are walking pets in the park? How many people per hour, walking speed, which way are they going, What % of people are stopping? IMPACT\nWe were able to run analytics on the video feed and the derived variables. Also, we were able to generate a digital twin using the converted coordinates. This helped us build capability for any future projects that might entail the use of these technologies.\n","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"1befb9473e8b3c5e344d231478197eca","permalink":"https://prasang-gupta.github.io/project/worldsio/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/project/worldsio/","section":"project","summary":"AIM\nThe aim of this study was to test out some of the features offered by Worlds.io and create a generalised digital twin solution that can be leveraged in future projects.","tags":["IoT","Worlds.io","Digital Twin","Simulation Modelling","Deep Learning","Proof of Concept","ML-DL"],"title":"Worlds.io","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this project was to generate a solution that would auto-format any PPT in PwC-compliant format. The changes included several editorial changes (word alternatives, punctuations) and branding changes (colors, formatting). It also included aligning any misaligned objects present in the PPT.\nDETAILS\nWe decided to edit the underlying XML format of the presentation (using the open office lxml format) in addition to actually editing the PPT itself. We wrote code to parse the PPT in an editable format and then created a modular structure to work on different portions of the PPT. Some modules were rule-based, some were logic driven based on the requirements and some modules incorporated ML solutions developed for sub-problems wherever possible. Some of the modules built were:\nModule Type Function and Details Word Editorial Performed changes on the word level. One was making them consistent in terms of American English / British English. Also included removing any risk words and replace jargons with better suited alternatives Numbers Editorial Performed date parsing, currency conversions etc based on the format expected. Also changed numerical numbers to text wherever applicable Punctuation Editorial Added and modified punctuation marks wherever applicable in a consistent format Paragrapsh Editorial Identified any lengthy paragraphs or capitalisation issues in the presentation and gave suggestions to shorten it Font Branding Changed font sizes, styles and colors based on the location of the text (header, help box, content box etc) Bullets Branding Formatted simple and nested bullets to follow a particular pattern and adjusted font size and bullet marker according to the context Header and Footer Branding Adjusted header and footer in the master slide to be put on every slide in the document. Also, detected and accounted for repeated non-aligned headers and footers Pictogram Branding Detected images present in the slides and checked whether they are approved pictograms or if they infringe any copyright claims Colors Branding Changed the colors (font, background, pictograms) to be replaced with the nearest PwC-approved colors based off of a novel color matching technique Animations Branding Detected and changed any animations in the presentation wherever applicable IMPACT\nThis solution built was deployed on an internal hosting service and was made available as a service. The total processing time for an average presentation was about 5 minutes with all the modules active. This brought down the time to manually review and format average presentations from 30 minutes to about 10 minutes. The solution was not perfect, but it helped ensure that most of the repetitive tasks are taken care of by the code and only final inspection with some modifications need to be done by the manual reviewers, saving thousands of manhours for the firm.\n","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"086d97ef98c46eada4e2ef8c97baa82c","permalink":"https://prasang-gupta.github.io/project/yvce/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/project/yvce/","section":"project","summary":"Built a solution that transforms a non-formatted PPT file to a formatted PwC-compliant file including several Editorial, Branding and Alignment modules using lxml open office format","tags":["Automation","Formatting","Open Office XML (lxml)","Machine Learning","Firm Internal","ML-DL"],"title":"Automated PPT content editor","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this study was to build a system to distribute compute load across different resources present on the edge. It also involved testing the built system with a dummy exercise and find out the robustness of the system and any additional points that might need to be taken care of before actually attempting a client project in this domain.\nDETAILS\nWe built out a system for distributed edge compute using microk8s, a micro kubernetes engine. We paired it with metalb, a load balancer and dashboarding, resource tracking and reporting tools like grafana and prometheus. To simulate an edge environment, a local ensemble of devices was created and connected together on the same network. A total of 3 devices were included in the network including a raspbery pi, a windows laptop and an edge compute appliance. For testing out the pipeline, we ran a simple object detection model on all the devices using tensorflow serving.\nWhile testing, we also came across several limitations of the architecture. The first is that the compute power was very skewed within the networks (x86 devices were orders of magnitude faster than ARM). This lead to bottlenecking at times even with the load balancer. We also realised that running a full object detection model on small devices was not a great idea due to the compute and memory limitations of each device.\nIMPACT\nThe system built was a demonstration that multiple unused devices can be pooled together at the edge to increase the overall compute capabilities, reducing the lag of transferring data to and from the cloud. Apart from reducing latency, it also helps in keeping the data secure as everything is happening locally. This also ensures that no single edge device is overutilised hampering the pipeline.\n","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"9070eaca88ccc0748ab7d03dca8a64ef","permalink":"https://prasang-gupta.github.io/project/distributededge/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/project/distributededge/","section":"project","summary":"Built a system capable of distributing compute across different devices on edge using load balancing and micro kubernetes engine","tags":["IoT","Distributed Compute","Compute at Edge","Proof of Concept","ML-DL"],"title":"Distributed Edge Compute","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this study was to predict consumer demand using data from competition and infusing it with other datasets that influence demand like mobility and demographic variables. The other goal was to correct a proprietary mobility dataset used by PwC to incorporate for mobility changes brought on by COVID restrictions.\nDETAILS\nThe proprietary dataset mentioned had several flaws including data inconsistencies over months and years and noisy data which was not helping in developing stable models. It also didn\u0026rsquo;t account for any socio-economic variables based on the demographics of the region. To correct this, the proprietary dataset was merged with demographics variables based on the location and some feature engineering was done to get the mobility in radii of 1km to 5km based on the store location. The final sales value was kept as the dependent field for the model.\nMultiple model architectures were tried to model this information to correct the mobility, including ML and DL models. Finally, a voting ensemble method was selected which was modelling the corrected demand with an accuracy of about 70%.\nTo bump up the numbers further, an aerial snapshot of the region around the score was obtained from Google Satellite API at a tuned zoom level and a segmentation exercise was performed on the image to extract the type of region around the store. This was expected to bring another dimension of immediate demographics in the dataset. This was done by extracting the coverage area of different colors based on pixel masks :\nGrey (signifying roads, parking lots, streets, etc) Green (signifying agriculture, parks, vegetation, etc) White (signifying roof-tops, buildings, etc) IMPACT\nThe final model with all the variables included (engineered visits from proprietary data, Google mobility data, demographics data and derived immediate demographics of the store from satellite images) achieved a test accuracy of 75%. This improved the quality of the mobility data and impacted tens of projects that utilised this mobility dataset helping in building much more accurate and robust models for client deliveries.\n","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"625119bbccb4d9258bf32192331e94b8","permalink":"https://prasang-gupta.github.io/project/demandprediction/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/project/demandprediction/","section":"project","summary":"Predicted demand using data from competition and other variables such as demographic and footfall variations in the region.","tags":["Deep Learning","Active Learning","CIFAR10","Proof of Concept","ML-DL"],"title":"Demand Prediction with Competition Analysis","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this project was to build out a fully functional proof of concept for the fingervein detection biometric system. This end-to-end system would include a frontend for new user registration, verification of user fingervein and deletion.\nDETAILS\nWe achieved this using a Fingervein device and a custom built secure backend database. All the services were defined as part of an API which was hosted on a cloud platform. All the information transfer was done after using encryption. Furthermore, the frontend was built as a Google Chrome extension and integrated right into the browser for easy usage. This would ensure scalability and hassle free adoption.\nIMPACT\nWe were able to test out the full end-to-end pipeline right from registering new users and other management options provided to actually verifying the users based on the fingervein scans. This capability was built out and documented to be used for any future client engagements as a replacement for the traditional fingerprint security system.\n","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"a140b0e3852815188e78345ef2310770","permalink":"https://prasang-gupta.github.io/project/fingervein/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/project/fingervein/","section":"project","summary":"Designed an end-to-end biometric system with user registration and verification as a replacement for fingerprint technology","tags":["IoT","Fingervein Detection","Biometrics","Security","Proof of Concept"],"title":"Fingervein Detection","type":"project"},{"authors":null,"categories":null,"content":"We achieved 28th rank in the hackathon.\nAIM\nThe aim of L2RPN (Learning to Run a Power Network) 2020 hackathon - Robustness Track was to build a robust agent that would be keep delivering reliable electricity everywhere and also operate the grid safely when an agent takes unknown adversarial actions at regular intervals.\nDETAILS\nThe dataset contained episodes spanning different adversarial actions taken by the agent. The agent could terminate any 1 of the 10 possible power lines (some of them being high voltage lines). Our agent was to be evaluated on how long it can provide reliable power to consumers without causing blackout. Once a blackout occurs, it is game over for that episode. The operation cost to be minimized included powerline losses, redispatch cost and blackout cost.\nEvery substation in the competition grid had a \u0026ldquo;double busbar layout\u0026rdquo;. Hence, there was a choice of bus for making a connection from one of the bus to a grid object. Due to this and the medium sized grid used, the action space was of the order of 10^5 with the combinatorial action space reaching infinity.\nOur approach was derived from a previous public solution for this competition. THe idea was to reduce the action space and train 2 A3C models with different training parameters.\nIMPACT\nWe managed to improve upon the baseline set and managed to achieve a score of 10.84, which was the 21st highest score on the leaderboard, gaining us 28th rank.\n","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"23c97ebe65cb712bf622f6e67716c39f","permalink":"https://prasang-gupta.github.io/project/l2rpn/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/project/l2rpn/","section":"project","summary":"RL-based challenge to robustly maintain an electrical grid without disruptions against an adverse agent. Achieved 28th rank in the hackathon","tags":["Power network","Adverse agent","Reinforcement Learning","CodaLab","Hackathon","ML-DL"],"title":"L2RPN Hackathon 2020 - Robustness Track","type":"project"},{"authors":null,"categories":null,"content":"PROBLEM\nThe client wanted to prepare for and ensure a safe transitioning of people from \u0026ldquo;Work from home\u0026rdquo; to Office space. They wanted to make sure that the facility should be used responsibly and at no time there should be breaches of the social distancing regulations.\nSOLUTION\nWe prepared a solution that gave the client the ability to track and measure the occupancy and social distancing norms anonymously. We set up a LIDAR in the client office space attached with a hub to send the data to the cloud. Several calculations and checks were performed on the cloud and the final data was sent to the dashboard. The real time occupancy readings (both overall and zone-based) were also visualised by the dashboard.\nIMPACT\nAs a result of our solution, the client could monitor occupancy in real-time, track occupancy trends and monitor the hotspots in the office. As an extension, this can further be used to optimise the office space usage.\n","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"4947beace9c94a6aff811f12d9eaa240","permalink":"https://prasang-gupta.github.io/project/occupancydetection/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/project/occupancydetection/","section":"project","summary":"Built a solution for anonymised occupancy detection inside office space using LIDARs with features including social bubble breach detection and zone-based real-time occupancy counts and tracking","tags":["IoT","Occupancy Detection","LIDAR","AWS Cloud","Client Delivery"],"title":"Occupancy Detection","type":"project"},{"authors":["Prasang Gupta","Swayambodha Mohapatra"],"categories":null,"content":"","date":1600732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600732800,"objectID":"23aacb2139455ca977bd790a0de66209","permalink":"https://prasang-gupta.github.io/publication/clef2020/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/clef2020/","section":"publication","summary":"11th International Conference of the CLEF Association (CLEF 2020) - Thessaloniki, Greece","tags":["HTML","UI","Image Processing","Deep Learning","OpenCV","Mask-RCNN","Multi-Pass Inference"],"title":"HTML Atomic UI Elements Extraction from Hand-Drawn Website Images using Mask-RCNN and novel Multi-Pass Inference Technique","type":"publication"},{"authors":null,"categories":null,"content":"PROBLEM\nOur client was facing unplanned machine downtimes in their factories. Obtaining insights from machinery on factory floors is time-consuming, complex, and costly due to legacy infrastructure and bespoke systems that cannot be easily accessed. Accelerating the time it takes to draw intelligence from machine data is critical to get an accurate pulse on manufacturing operations. While many vendors offer comprehensive IIoT solutions, we were looking for quicker and more cost-effective alternatives to deploying a sophisticated end-to-end platform.\nSOLUTION\nWe designed, deployed and tested a fully-functioning prototype on our client’s manufacturing floor in less than 4 weeks. Our solution utilized low-cost vibration sensors and accelerometers connected directly to selected machines. These sensors captured near real-time data and provided rapid analytics by bypassing timely integrations with existing factory systems.\nWe also designed and built a customized dashboard offering a simple and elegant view of the captured insights. The insights present on the dashboard included machine schedules, unplanned downtimes, overall availability and the number of times the machine was stopped. All of this was reported and updated real-time with a resolution of 4 minutes.\nIMPACT\nInstead of spending months and significant budget to deploy an end-to-end IIoT platform, our client was equipped with critical insights in a much shorter timeframe. KPIs captured included machine up/downtime compared with scheduled operating times. The intelligence gathered not only provided efficiency gains and cost savings, but also avoided our client complex work orders to deploy sensory equipment. Consequently, client involvement was low-touch and the manufacturing process was not interrupted.\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"352596110b6f023a936dc0c7ef3d88cc","permalink":"https://prasang-gupta.github.io/project/factoryintelligence/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/project/factoryintelligence/","section":"project","summary":"A smart factory solution with unplanned machine downtime detection and industry-standard OEE calculations using cost-effective sensors and a dashboard with key KPIs","tags":["IoT","Factory Intelligence","Unplanned downtime detection","AWS Cloud","Low Code AEP","Client Delivery"],"title":"Factory Intelligence","type":"project"},{"authors":null,"categories":null,"content":"We achieved 3rd rank in the hackathon.\nAIM\nThe aim of this hackathon was to localise and identify several different HTML UI elements in hand-drawn wireframe drawings of websites.\nDETAILS\nThe dataset provided for the hackathon contained about 3000 wireframe drawings of websites. The goal was to identify the different HTML UI elemnents present in the image, such as \u0026ldquo;Text Box\u0026rdquo;, \u0026ldquo;Button\u0026rdquo;, \u0026ldquo;Image\u0026rdquo;, etc. Hence, it boiled down to an object detection problem.\nWe tried using several Object Detection algorithms like YOLO, R-CNN and Mask-RCNN and decided on Mask-RCNN as it was providing us with the best results. However, one thing we observed in our outputs was that our Precision scores were good, but the model was lacking in Recall bringing the whole F1 down. To solve this problem, we came up with a novel technique \u0026ldquo;Multi-Pass Inference\u0026rdquo; that booosted our recall scores.\nThe technique involves running the image through the model multiple times, each time taking note of the objects that are already detected and removing them for subsequent passes. This forced the model to predict more instances of the elements in the image. We smarlty combined the objects detected in multiple passes to overall boost the recall score of our model helping us to take a podium spot in the leaderboard.\nIMPACT\nThe solution built was performing really good on unseen test images managing an mAP (IoU \u0026gt; 0.5) score of 64.12. This solution was later implemented into a pipeline to allow rapid prototyping of websites and dashboards.\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"22660754dbcd94c3f2cdbc5c7fd5ded8","permalink":"https://prasang-gupta.github.io/project/wireframeuidetection/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/project/wireframeuidetection/","section":"project","summary":"Extracted HTML UI elements from wireframe drawings of websites ideating novel multi-pass inference technique to boost recall. Achieved 3rd rank in the hackathon.","tags":["HTML","UI","Image Processing","Deep Learning","OpenCV","Mask R-CNN","AICrowd","Hackathon","ML-DL"],"title":"HTML UI element extraction","type":"project"},{"authors":null,"categories":null,"content":"PROBLEM\nThe client were facing problems with cargo damage while in transit which were resulting in huge losses for them. Also, since the transit included multiple contractors responsible for different stretches of the overall route, and nobody was taking responsibility for the damages, they were at a loss and had no idea how to rectify this.\nSOLUTION\nWe created an end-to-end asset tracking system for the client. The clients were given low-cost devices housing different sensors to include them with their shipments. These devices were very small (about 2cm x 2cm x 1cm) and were tested for harsh weather. Also, these devices were equipped with a battery, a GPS and cellular communication technology along with the accelerometer, humidity and temperature sensors. These were configured to be used as one time use devices to save on device recovery costs / logistics. They were configured to report information all along the journey and last the whole shipment time without the need of a recharge.\nThresholding was done on the accelerometer sensor housed in the device to store high impacts faced by it, which would directly correlate with the damage faced the equipment. These raw accelerometer values were then configured to be sent to the cloud managed by the manufacturer. This data was then available to be used using simple API requests.\nThe raw data collection and conversion to impact system was built on an application enablement platform. This resulted in quick prototyping and testing of the proof of concept. The processed data was then sent to a live dahsboard which was shared with the client for their perusal. This dashboard offered several important metrics such as the last known location, temperature and battery of the device. It also showed the impact values versus timestamp which were recorded by the device.\nIMPACT\nFrom using this solution, we were able to ascertain different impact values faced by the device during different portions of the journey. This helped the client to visualise and piece together possible regions where the shipment was getting damaged. Our analysis found that most of the impacts registered by the device were when the device was travelling from the factory to the ports, which was contrary to the earlier belief of the client that most of the damage was being caused during the ship route. This allowed the client to focus on that section and helped them save a lot of revenue spent on replacing damaged products as well as save the reputation of the company.\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a5a2fcfbae6947872820b3e442812000","permalink":"https://prasang-gupta.github.io/project/assettracking/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/project/assettracking/","section":"project","summary":"Tracked cargo shipment using cost-effective sensors along its cross-country journey from factory to distribution center ascertaining possible locations of damage for effective preventive steps","tags":["IoT","Asset Tracking","Cost-effective sensors","Logistics","Low Code AEP","Client Delivery"],"title":"Cross-country Asset Tracking","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThe aim of the study was to build out a pipeline for active learning that other projects with scarce labelled data will leverage for model building. It also included trying the effectiveness of active learning on a standard dataset and evaluate different techniques.\nDETAILS\nThe dataset chosen for this study was CIFAR10. This dataset contains 60,000 32x32 images with 10 unique classes. We used a maximum of 10,000 images at each time for training purposes. This study\u0026rsquo;s focus was to compare the performance of active and passive learning using same number of labelled samples.\nThe passive learning model would be trained on a uniform distribution of the dataset size, however, the active learning model would be initialised with half the training size and then would be trained for the full size by sequential queries. We tried 3 queries in this study : uncertainty, margin sampling and entropy sampling. We achieved better performance with active learning for all the sizes, however, the improvement in performance varied from as low as 1% to a sizable 23%. The details for all the experiments and the queries can be seen in the slides.\nIMPACT\nThe pipeline developed from this project was consecutively used in multiple client projects to improve the modelling capabilities and thrive wherever manually labelling the full dataset was not an option.\n","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"69979d71a9f34f8709e0350fd7c3f097","permalink":"https://prasang-gupta.github.io/project/activelearning/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/activelearning/","section":"project","summary":"Explored Active Learning with different querying strategies on CIFAR10 dataset and managed to achieve high accuracies with very limited training data","tags":["Deep Learning","Active Learning","CIFAR10","Proof of Concept","ML-DL"],"title":"Active Learning","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this project was to consolidate the different trainings held in the firm for the last 3-4 years and generate a searchable dashboard with relevant information.\nDETAILS\nThe consolidation and standardisation for all the trainings was done by scraping information from mails and other portals and arranging them in google drive based on the scraping output. A live tableau dashboard was then generated from the index sheet and deployed on a central platform that everyone in the firm can access. Apart from keyword search, multiple filters were also included in the dashboard.\nIMPACT\nThis dashboard allowed people in the firm to search for past trainings with feedback and help plan future trainings based on that. Also, several hours were saved allowing people to re-use decks and other material from past trainings. This also helped foster an environment of self-learning within the firm as one can just search and get all the material relevant for a particular topic in just seconds.\n","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"bcdf3eceacdef058d050392b64de73bd","permalink":"https://prasang-gupta.github.io/project/lndrepo/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/lndrepo/","section":"project","summary":"Collected all the firm internal / external trainings in one place and put a Tableau Live Dashboard on top of the data.","tags":["Tableau","Firm Internal","Firm Development"],"title":"Trainings Repository","type":"project"},{"authors":null,"categories":null,"content":"PROBLEM\nThe client wanted to reduce the time spent by their employees in looking through several different photographs submitted for insurance claims clearing and ascertain damaged parts of the vehicle with the extent of damage.\nSOLUTION\nWe solved the problem by training a semantic segmentation model used for ascertainining the different kinds of damage that were present in the photograph of a vehicle (like the figure attached). To ensure the correctness of the model, we also employed an explainable AI technique, LIME, which returned the parts of the image it is looking at when coming to a decision about the damage of a particular type.\nA few classification models were also trained to fetch images which were visually similar with the current image. The different variants were the similar damage model, where the model would return the top images which have visually similar damage to the current image. The other variant was the similar non-damage model, in which the model would return the top images of similar vehicles of previously processed claims.\nTo top it all off, an automated report generation tool was coupled with the model (using FPDF), which returned a formatted PDF report having detailed information regarding the damage and the claims (a sample report attached).\nIMPACT\nThe segmentation model ensured that the client team spent lesser time on figuring out the damage and more time providing personalised support to the consumers. The classification models helped the client team to look at some previous claims to decide the outcome for the current claim in a more informed and consistent manner. This improved the overall reputation of the firm in disbursing out claims. Also, the automated report could be handed over to the consumers directly for a much more transparent view into the claims processing.\n","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"47148c388f41cabf2e022c8374cce312","permalink":"https://prasang-gupta.github.io/project/carpartsegmentation/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/project/carpartsegmentation/","section":"project","summary":"Performed damanged car parts segmentation for automating auto claims with additional features like explainability and automated claim / damage report generation","tags":["Deep Learning","Semantic segmentation","Classification","Auto insurance","AI assisted claims","Report Generation","Client Delivery","ML-DL"],"title":"Damaged Car Parts Segmentation for auto claims","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThe aim of this project was two fold. The first aim was to implement an action recognition model and the second was to modify it to run on an edge device. For this, we chose the Pre-trained Temporal Relation Network Model. The dataset chosen for this was the 20BN-something-something Dataset V2. This dataset has over 100 classes of different object-human or object-object interactions.\nDETAILS\nThe model was first implemented on a laptop with the webcam and then later extended onto the edge device, Jetson TX2. Prior to this, the TX2 was flashed and proper libraries were built from source to enable it to use its full potential (CUDA cores for rendering). We were successfully able to implement this on the board and were getting very respectable frame rates, somewhere around 10 fps. The performance of this model for several different scenarios can be seen in the videos link and a little detail about the implementation can be found in the slides.\nApart from this, we also implemented simple object detection models on Raspberry Pi on which we were getting around 1-2 fps.\n","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"19a26118ba4a1022596252901701b460","permalink":"https://prasang-gupta.github.io/project/edgelivemodels/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/project/edgelivemodels/","section":"project","summary":"Implemented and optimised multiple live models (eg. action recognition on something something dataset with over 200 classes) on edge devices (eg. Jetson TX2, Raspberry Pi)","tags":["IoT","Deep Learning","Edge Devices","AI on Edge","NVIDIA Jetson TX2","Raspberry Pi","Live Action recognition","Something Something Dataset","Proof of Concept","ML-DL"],"title":"Implementation of Live models on Edge","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThis was a course project for Chemical Desgin course under Prof. Nitin Kaistha. The aim of the project was to design a chemical plant from scratch for the production of acrylic acid. The design should be optimal with respect to the different costs associated with the production and it should be robust to input stream fluctuations. This was a rather interesting project as it brought together everything that we learned in the span of 3 years.\nDETAILS\nThe design was meticulously thought of by theoretical knowledge and the whole plant was simulated using ASPEN HYSYS package. The simulations were done both for the equilibrium stage of the plant as well as for the dynamics with controllers attached to several parts of the components, as can be seen in the figure.\n","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"a45e2c7f0ff17f65abee46f11e199b40","permalink":"https://prasang-gupta.github.io/project/che453/","publishdate":"2017-11-01T00:00:00Z","relpermalink":"/project/che453/","section":"project","summary":"Designed, simulated and optimised a complete plant design for the production of acryclic acid with raw materials on ASPEN HYSYS and MATLAB. Successfully tuned the plant for optimal performance with minimum cost and robustness to raw material stream fluctuations.","tags":["Chemical Engineering","Plant Design","Acrylic Acid Production","Controls","ASPEN HYSYS"],"title":"Complete Plant Design of Acrylic Acid","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThis project was part of my internship at TATA Research Design and Development Centre. The internship started May 2017 and ended July 2017. The objective of the project was to develop a macroscopic model of skin’s upper layer stratum corneum using OpenFoam, an open-source library. This library was chosen as this provided and open source FVM alternative to the licensed FEM solver, COMSOL. Apart from that, nobody in the firm had used it before, so it served as an exploration study into using this library. However, as I was able to finish the objective early, I also finished an extension of the project, which included studying the effects of electroporation on the drug transfer kinetics.\nDETAILS\nThe first task was to get used to the OpenFOAM library which is completely CLI based. After getting a handle of the different methods and way of working, the first model was developed of the uppermost skin layer, Stratum Corneum using a parameterised brick and mortar structure. This model was then used to model a transdermal drug delivery by Fickian diffusion through the layer.\nFentanyl and Caffeine were chosen to be studied based on literature review and their scope of usage in transdermal delivery. The diffusion coefficients for these components were obtained from MD simulations. Running them through the developed model provided drug release profiles for both. These profiles were validated by experimental studies present in literature.\nSince, there were a lot of files that needed to be modified to set up an OpenFOAM simulation, running iterations while maintaining correctness (not missing editing a file) was a steep task. Hence, I worked on automating the whole simulation process and file editing structure using C++ and Bash. This reduced simulation set up times drastically allowing me to run several iterations and complete the objective of my internship way before my internship end date.\nIMPACT\nI was able to extend the study to include the effects of electroporation on the drug delivery profiles. The new model developed with electroporation was again validated with experimental data and was found to be in good agreement. Also, the drug release significantly increased after using electroporation which was expected from the study.\n","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"065865e70f080dfc2cb70a483d61ac37","permalink":"https://prasang-gupta.github.io/project/transdermal/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/project/transdermal/","section":"project","summary":"Developed a macroscopic model of skin's upper layer (Stratum Corneum) to study transdermal drug delivery and effectiveness of electroporation on drug transfer.","tags":["Chemical Engineering","Transdermal Drug Delivery","Electroporation","Skin modelling","OpenFOAM","C++"],"title":"Macroscopic model for Transdermal Drug Delivery","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThis was an Undergraduate project under Prof. Naveen Tiwari, Prof. V Shankar and Prof. Goutam Deo from January 2017 to March 2017. The aim of the study was to model an SiO2 burner with specific dimensions and study the temperature profiles of the burner. The project also included studiyng the distribution of different species present at different distances away from the flame.\nDETAILS\nTo kick off the modelling, adiabatic flame temperature was calculated for a hydrogen-oxygen combustion process. The equilibrium analysis was done on ASPEN Plus package. To validate the results, the same value was cross-referenced with literature values. It was found to be in good agreement with that. Next, a 100% conversion analysis was done using the same suite. However, this time the same model was built using first principles on MATLAB and the results were compared. Again, there was a very good agreement with both of these values. Hence, we concluded that the modelling process done in the ASPEN Plus suite is correct.\nHaving validated our model, we started experimenting with different conditions for the same process. This included changing the temperature as well as adding Silicon Tetrachloride in the feed. Finally, a dependence between the hydrogen flow rate with the adiabatic temperature was established.\nNext, we moved on to the kinetics of the problem. The overall reaction was broken down into several different intermediate reactions. Every one of these intermediate reactions have different kinetic parameters. All the possible intermediate reactions were written and validated with theory and literature. Preliminary reaction kinetics were performed with an assumed value of burner diameter and were qualitatively checked with literature values.\nIMPACT\nHaving a validated model ready for use, the original burner dimensions used by the industrial partner were added to the model and all the calculations were re-run for this. The mole fractions of the different species were studied at the end of the flame. This provided much better view into the temperature variation in the flame and the different reaction progress with the length of the flame. This gave the industrial partner insights into the minimum length of the flame to ensure that the reaction gets over and a maximum conversion of raw materials to products can be achieved with minimal waste.\n","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"52259abf97ccb4a6e46c9408e9cced1b","permalink":"https://prasang-gupta.github.io/project/burnerprofile/","publishdate":"2017-04-01T00:00:00Z","relpermalink":"/project/burnerprofile/","section":"project","summary":"Performed a numerical study of SiO2 burner profiles simulating a burner flame and calculating the flame temperature profile and species distribution in the flame.","tags":["Chemical Engineering","Burner profile","Adiabatic Temperature Calculation","MATLAB","COMSOL","ANSYS Chemkin","ASPEN Plus"],"title":"Numerical Study of SiO2 Burner profiles","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThis was an Undergraduate project under Prof. Nishith Verma which went on from March 2016 to December 2016. Our aim was to perform an investigation into the particle densities obtained by mixing two fluids in a Y-shaped mixture.\nDETAILS\nAs a first step, the y mixer model was generated in C++ in 2 and later 3 dimensions and simulations were run using the Lattice Boltzmann method. This resulted in fluid flow diagrams which were matched with experiments. After validating this model, tracer analysis was done on the outputs. This ensured us to capture the cross section and the mixing capabilities of the mixer. This was again validated from experiments.\nHaving a C++ lattice boltzmann model ready, we moved on to using the palabos library to include population balance equations in our simulation model. This included the new calculations as well as sped up the entire code by about 40%. Nucleation rates and growth rates for the different particles were provided based on experimental chemical reactions and the population density of the product was studied at different cross sections of the mixer representing varying mixer sizes.\n","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"6bb36ab7b26abb182522c38b0f6db3e2","permalink":"https://prasang-gupta.github.io/project/ymixer/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/project/ymixer/","section":"project","summary":"Performed a simulation study of nanoparticle precipitation in 2D and 3D Y-mixers solving the population balance equation. The study included fluid flow and tracer analysis using both C++ and palabos library.","tags":["Chemical Engineering","Population Balance","Nanoparticle precipitation","palabos","C++"],"title":"Numerical Investigation of Nanoparticle Precipitation in Y-Mixers","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThis was a part of the Unit Operations Lab course. The aim of this project was to innovate and go above and beyond the standard lab experiments with the same equipment.\nDETAILS\nWe modelled the whole process in MATLAB Simulink deriving transfer functions from experimental data. Also, we took the default PID values present in the controller on the setup and tuned those values. We found that the response received is much better than the default response recorded. Hence, we submitted the parameters to be updated in the controller for more accurate experiments thereafter.\n","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"327060ab581162c76e764187b2d3e35b","permalink":"https://prasang-gupta.github.io/project/innovationlabpid/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/project/innovationlabpid/","section":"project","summary":"Studied the control characteristics of a process with temperature scaling. Tuned the parameters using PID tuning on MATLAB.","tags":["Chemical Engineering","Controls","Transfer Function","PID Tuning"],"title":"PID Characteristics","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThis was a voluntary project under the Aeromodelling Club in the summer holidays from May 2016 to July 2016. The aim of the project was to train a team to build a composite glider as well as try a new kind of HLG glider, the Allegro 2m.\nDETAILS\nAs mentioned, the composite glider \u0026ldquo;Allegro 2m\u0026rdquo; was chosen for this project. The different airfoils were directly obtained from Prof. Mark Drela\u0026rsquo;s page and templates were cut using laser cutter. These templates were then used to cut the foam which would form the base of the wings / tails using a gravity hot wire cutter. These foam cuttings were then covered with an optimal number of glass fibre and carbon fibre sheets for providing it hte strength it needs. The optimum layers were calculated by designing a MATLAB code to take into account shear stress at different locations of the wing while in flight.\nContary to the previous glider that was developed, this glider was also polyhedral but there was no dihedral in the middle and there were 2 angles on each side totalling 4 dihedrals. This ensured much better construction as giving a dihedral in the middle is very tricky. These were carefully built to maintian the same angle to ensure roll stability. The hollow fuselage was developed from the previously developed moulds. The hollow tail was built using cross arranged rolled carbon fibre cloth. Lastly, the wings and tails were spray painted and then all of the components were assembled together.\n","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"f7791778554f29edbadf61ad83634be6","permalink":"https://prasang-gupta.github.io/project/glidermentor/","publishdate":"2016-07-01T00:00:00Z","relpermalink":"/project/glidermentor/","section":"project","summary":"Mentored and led a team of 5 people to fabricate Prof. Mark Drela's Allegro glider (Hand Launch Glider) with a 2m wingspan","tags":["Aeromodelling","HLG (Hand Launch Gliders)","Composite glider","XFLR","Hobby"],"title":"Composite Allegro 2m glider","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThis was a part of the Unit Operations Lab course. The aim of this project was to innovate and go above and beyond the standard lab experiments with the same equipment. We were assigned the centrifugal pump setup which is originally used to study the effect of series and parallel setup of pumps on the discharge pressure and flow rate. We however decided to perform a very different study using the same equipment.\nDETAILS\nDuring our experiments, we noticed that there was a measurable gap between turning up the RPM of the motor and actually observing differences in the flow rate. This lead us to believe that there is some sort of a lag in play between the inputs and outputs. To model this lag, we assumed first order dynamics for simplicity and calculated the transfer function of the pump ny oberving the time difference it takes to reach equilibrium after giving the input. We later tested this obtained transfer function by further experimenting with the setup.\nTo ensure utmost accuracy in our calculation, we used high FPS cameras to record readings when we were giving a small step change as an input to the motor (change of RPM). We also performed an opimisation study for an optimum RPM for the motor for best energy efficiency.\n","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"5539ee4f363e92280dbed44384db1dcc","permalink":"https://prasang-gupta.github.io/project/innovationlabpump/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/project/innovationlabpump/","section":"project","summary":"Studied the control characteristics of the centrifugal pump system. Derived the transfer function for flow rate and discharge pressure from the response of a step change in RPM and verified the same by further experimentation. Also, calculated the optimum RPM operation point for the Pump setup.","tags":["Chemical Engineering","Controls","Transfer Function","Centrifugal Pump characteristics"],"title":"Centrifugal Pump Characteristics","type":"project"},{"authors":null,"categories":null,"content":"We achieved 3rd rank among the 105 teams that participated from different technical institutions from all across India.\nAIM\nROBOCON is an Asia-Pacific robotics competition held every year with different problem statements. The problem statements are carefully curated to encourage design innovations in the field of robotics. The theme of the competition for the year 2016 was \u0026ldquo;Clean Energy recharging of world\u0026rdquo;. The problem statement was to make 2 robots, one being a powered robot and the other being a non-powered robot that will move using non-contact forces provided by the former. The aim was to make the non-powered robot travel on a path with elevation, curves, sharp turns etc autonomously using line following. This bot was carrying a lightweight fan. The other robot, in addition to providing the required force for the non-powered robot to move, had to collect the aforementioned fan from the non-powered robot after it had covered the entire stretch, climb a pole and then place the fan on the top of the pole. This would mark the completion (\u0026ldquo;Chai-Yo\u0026rdquo; as they called it) of the challenge and the team that achieves this first would win.\nDETAILS\nI was one of the leaders in the 21 member team that took part in the competition represting my alma mater, IIT Kanpur. We worked on this from October 2015 to March 2016. We managed to achieve that participated from different technical institutions from all across India.\nWe chose to go with wind as the mode of force to move the non-powered robot. We used a propellor fan to generate the wind and a styrofoam sail was attached to the non-powered robot to catch the wind and propel it forward. This bot was made using lightweight balse wood and glass fibre which have an impressive strenght-to-weight ratio. We further designed the truss patterns on the wood and got it cut by laser to have the lightest possible bot. We also employed line tracking on the bot to follow the required path. These prototypes were first designed on Autodesk Inventor or Solidworks and then cut and assembled using precise machinery.\nThe powered bot had an autonomous climbing mechanism using rubberised clamp wheels. To follow the path and align itself to the pole, it used wall following to follow the arena wall. We were consistently able to achieve \u0026ldquo;Chai-Yo\u0026rdquo; in about 50 seconds of runtime (You can take a look at the video which has the complete run while we were still preparing for the competition).\n","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"4ea3cf7e5a8663511adae3ca8118c1e5","permalink":"https://prasang-gupta.github.io/project/robocon16/","publishdate":"2016-03-01T00:00:00Z","relpermalink":"/project/robocon16/","section":"project","summary":"Designed 2 robots as a part of the ABU ROBOCON 2016 challenge, one being a powered robot and the other being a non-powered robot that will move using non-contact forces provided by the former and steer using line following. The powered robot was equipped with wall following and pole climbing. Achieved 3rd rank nationally out of about 105 teams.","tags":["Robotics","CAD Design","Lightweight construction","Line / Wall following","Pole climbing","Hobby"],"title":"ABU Robocon 2016","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThis was a voluntary project under the Aeromodelling Club in the summer holidays from May 2015 to July 2015. The aim of the project was to build a composite glider.\nDETAILS\nThe composite glider \u0026ldquo;Aegea 2m\u0026rdquo; was chosen for this project. The different airfoils were directly obtained from Prof. Mark Drela\u0026rsquo;s page and templates were cut using laser cutter. These templates were then used to cut the foam which would form the base of the wings / tails using a gravity hot wire cutter. These foam cuttings were then covered with an optimal number of glass fibre and carbon fibre sheets for providing it hte strength it needs. The optimum layers were calculated by designing a MATLAB code to take into account shear stress at different locations of the wing while in flight.\nThe wings of the aegea were polyhedral with a dihedral in the middle and 1 more angle on each side totalling 3 dihedrals. These were carefully built to maintian the same angle to ensure roll stability. The hollow fuselage was developed from moulds which were again in turn made from composite materials. The hollow tail was built using cross arranged rolled carbon fibre cloth. Lastly, the wings and tails were spray painted and then all of the components were assembled together.\nIMPACT\nThis was the first time that composite materials were being used in the Aeromodelling Club, IIT Kanpur. This project opened up a whole array of opportunities for the club to divest into composite construction of other designs including flying wings, UAVs etc.\n","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"78a982bd77f16794a93ea40d8ab206f4","permalink":"https://prasang-gupta.github.io/project/gliderself/","publishdate":"2015-07-01T00:00:00Z","relpermalink":"/project/gliderself/","section":"project","summary":"Fabricated a composite glider aegea (Composite Light Aileron Hand Launch Glider) with a wingspan of 2m based on design by Prof. Mark Drela.","tags":["Aeromodelling","HLG (Hand Launch Gliders)","Composite glider","XFLR","Hobby"],"title":"Composite Aegea 2m glider","type":"project"},{"authors":null,"categories":null,"content":"AIM\nThis project was done as a part of the summer project offering from Robotics Club, IIT Kanpur. The aim of this project was to build an all-terrain robot capable of autonomous navigation and provide surveillance in remote areas.\nDETAILS\nWe modelled the robot on Solidworks and machined it using aluminium and CNC machines. For navigation, we integrated the IMU sensor with the GPS anc coupled them together iwth a Kalman filter. Also, we imported Google Maps API for setting the route to be travelled and later converted the route into different waypoints to be followed. This waypoint file was uploaded to the robot\u0026rsquo;s APM (mission planner).\n","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"f9849292161002f6cd5dbd13607ee7a5","permalink":"https://prasang-gupta.github.io/project/sastr/","publishdate":"2015-07-01T00:00:00Z","relpermalink":"/project/sastr/","section":"project","summary":"Developed an all-terrain vehicle capable of automatic navigation and surveillance using Embedded systems and Google Maps API.","tags":["Robotics","CAD Design","Localisation and Surveillance","Hobby"],"title":"Semi-Autonomous Surveillance and Tranportation Robot","type":"project"},{"authors":null,"categories":null,"content":"We achieved 11th rank among the 80 teams that participated from different technical institutions from all across India.\nAIM\nROBOCON is an Asia-Pacific robotics competition held every year with different problem statements. The problem statements are carefully curated to encourage design innovations in the field of robotics. The problem statement for the year 2015 was to make 2 robots that will play a doubles badminton match with the other team\u0026rsquo;s robots. The choice of fully automated vs manual robots was provided by the organisers. I was a part of the 21 member team that took part in the competition represting my alma mater, IIT Kanpur. We worked on this from October 2014 to March 2015.\nDETAILS\nThe 2 bots designed were capable of serving the shuttle cock across the net to land in a specific region designated as a \u0026lsquo;successful serve\u0026rsquo; by the organisers. They also had mechanisms to return the serve of the opposing team using different placements of badminton racquets. The image shows one of our robots that had a double pneumatic actuation mechanism in the center alongwith 2 side racquets to return the shuttle.\nI was a part of the electronics team, managing the power delivery to the different components of the bot. Also, to reduce the clutter of wires, I designed Printed Circuit Boards (PCBs) of different components of the bot, eg. the motor driver, on Altium Designer and got them printed for use. I also had major contributions in designing the mechanism to serve the shuttlecock.\n","date":1425168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425168000,"objectID":"2c285ded34cfcda2f609613a4f1c5dc4","permalink":"https://prasang-gupta.github.io/project/robocon15/","publishdate":"2015-03-01T00:00:00Z","relpermalink":"/project/robocon15/","section":"project","summary":"Designed and developed 2 badminton playing robots (doubles on a standard badminton court) for the ABU Asia-Pacific Robot Contest 2015. Achieved 11th rank nationally out of about 80 teams.","tags":["Robotics","CAD Design","Badminton","ABU Robocon","Hobby"],"title":"ABU Robocon 2015","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://prasang-gupta.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]
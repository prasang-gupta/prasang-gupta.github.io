<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Prasang Gupta</title>
    <link>https://prasang-gupta.github.io/project/</link>
      <atom:link href="https://prasang-gupta.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Prasang Gupta</copyright><lastBuildDate>Mon, 01 Aug 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://prasang-gupta.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://prasang-gupta.github.io/project/</link>
    </image>
    
    <item>
      <title>Contract Lifecycle Management (CLM)</title>
      <link>https://prasang-gupta.github.io/project/contractlifecyclemgmnt/</link>
      <pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/contractlifecyclemgmnt/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;PROBLEM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The client wanted to switch from legacy contract management tool to a new vendor. However, they were not confident of the metadata they had for the contracts and wanted help in getting all the major information extracted from the documents. They also wanted hierarchical linkages for most of the contracts for better organisation in the new tool.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;SOLUTION&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We solved the first problem with a myriad of different tools. These included advanced NLP models for entity extraction based on transformers, tree based ML models for certain classifications and logic-driven string search models.&lt;/p&gt;
&lt;p&gt;Common fields that are present in almost every document were extracted using a trained NER model linked with an active learning pipeline. These models were built using a modified version of &lt;a href=&#34;https://github.com/princeton-nlp/PURE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PURE&lt;/a&gt;, an open source offering by Princeton NLP. The active learning pipeline was built using LabelStudio as a frontend for manual annotations to get initial training data and then subsequently, for getting labels for the most uncertain samples.&lt;/p&gt;
&lt;p&gt;Fields which had only a few possible values like &amp;ldquo;Agreement Type&amp;rdquo;, were trained using a simple Random Forest classifier. The training data was obhtained using a mix of string extractions from the filenames and the existing contract metadata provided by the client. This, combined with the NER model resulted in boosted accuracies for this field.&lt;/p&gt;
&lt;p&gt;We built logic-based string search models for those fields which were not abundantly present and were more situational. These included fields like &amp;ldquo;Force Majeure&amp;rdquo; and all fields associated with this, &amp;ldquo;Termination Payments&amp;rdquo;, etc. This helped us extract information for these fields for the majority of the documents they were present in with little overhead.&lt;/p&gt;
&lt;p&gt;All of these extractions were provided to the client and after spot-checking and QA, were used in the construction of hierarchical linkages. Strong linkages were built by extracting the reference phrase within contracts that contains the details regarding the parent or the child contract. These were then translated back to the database we had. Weak linkages were also built based off of logic to combine documents under each MSA umbrella wherever the extraction of the reference phrase wasn&amp;rsquo;t possible.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The extractions provided to the client passed the QA test with about 90% accuracy. Also, we managed to provide hierchical linkages for more than 75% of the documents provided to us that could be linked.&lt;/p&gt;
&lt;p&gt;This has historically been a manual-only job with several hours of manpower invested in reading through the huge contracts and extracting information. We accelerated this process by appending this with models wherever possible. This reduced the load on the manual annotators and we were able to complete this in a fraction of the time that would&amp;rsquo;ve been invested in an all-manual project &lt;span style=&#34;color:#5DADE2;font-style:bold&#34;&gt;saving roughly 30,000 hours and $750,000 for the client&lt;/span&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zero-Shot Deepfake Audio Generation</title>
      <link>https://prasang-gupta.github.io/project/deepfakeaudio/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/deepfakeaudio/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to create a demo for generating speech from text in the voice of the user given the least amount of sample possible.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As the aim of this study was to create a demo, we chose to go ahead with Zero-Shot model as it takes practically no time to train and can start giving good results even with very little amount of sample data. The tradeoff with accuracy was accepted in favour of time. We tested different models, the most popular being YourTTS, SCGlowTTS and SV2TTS. On the basis of qualitative assessment on the representative test dataset, which included both male and female audio samples in 3 different accents (American, British and Indian), YourTTS model was selected as the clear winner.&lt;/p&gt;
&lt;p&gt;After finalising the model, this was tested further for robustness and a minimum sample audio time was estimated based off of these tests and it came out to be 1 minute for getting decent results in most cases. We then deployed this solution on an internal hosting site and created an interactive demo using our developed model as the backend. A representative set of 10 small sentences were obtained which contain as many different phonetics as possible and then a feature was added to record the user speak these sentences.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After running these samples through the model, which takes about a minute, the user would be able to hear their deepfake speaking 5 different sentences that were not a part of the representative set in their voice. This rounded off a user-friendly demo that can be used for GTM strategies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intelligent Systematic Investment Agent</title>
      <link>https://prasang-gupta.github.io/project/rlisia/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/rlisia/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to explore Evolutionary strategies as an alternative to the classicaly used Reinforcement learning techniques to solve the ETF trading investment problem.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Historically, autonomous agents for trading and investment are generally built using one of the many reinforcement learning techniques. These include the SOTA Actor-critic algorithms as well as simpler DQN based algorithms. The problem with these is that an environment needs to be setup for training them and the learning curve for these algorithms is a bit steep for someone inexperienced in the field of Reinforcement Learning.&lt;/p&gt;
&lt;p&gt;We demonstrated that using Evolutionary strategies to solve episodic problems (which is getting daily purchase actions for a month in the current problem) can be a much easier and robust method. We also coupled this with a simple Neural network model to learn the patterns that are being revealed in the outputs of the ES runs. This broke down the problem into 2 mutually exclusive parts, which is much easier to understand and code.&lt;/p&gt;
&lt;p&gt;We compared both qualitative and quantitative metrics for our ensemble algorithm we call &lt;code&gt;GADLE&lt;/code&gt; with 2 traditional RL-based solutions, Actor-critic and DQN. Qualitatively, we compared the ease of writing of code and ease of understanding. Quantitatively, we ran different experiments centered around performance, sensitivity and consistency of the solutions.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proposed GADLE algorithm performs at par with the Actor-critic algorithm. However, it crushes the competition in terms of consistency and sensitivity. To put things in perspective, following are the tables summarising results of the sensitivity and the consistency experiments.&lt;/p&gt;














&lt;figure  id=&#34;figure-sensitivity-comparison-between-gadle-and-other-algorithms&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Sensitivity comparison between GADLE and other algorithms&#34; srcset=&#34;
               /project/rlisia/sensitivity_hu3a1eeebc4dcc2d597374b48d46a515c0_64132_22fa2060a5d243edc1acb6bcf95e40b6.png 400w,
               /project/rlisia/sensitivity_hu3a1eeebc4dcc2d597374b48d46a515c0_64132_b698d20a98ed40fe2b0c6afb9603e12a.png 760w,
               /project/rlisia/sensitivity_hu3a1eeebc4dcc2d597374b48d46a515c0_64132_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/rlisia/sensitivity_hu3a1eeebc4dcc2d597374b48d46a515c0_64132_22fa2060a5d243edc1acb6bcf95e40b6.png&#34;
               width=&#34;760&#34;
               height=&#34;253&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sensitivity comparison between GADLE and other algorithms
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-consistency-comparison-between-gadle-and-other-algorithms&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Consistency comparison between GADLE and other algorithms&#34; srcset=&#34;
               /project/rlisia/consistency_hu1ca179bcb047f7a1fbab8caa7874e21b_35741_4e41d84c91b3ce3cee113f3b57a93fe0.png 400w,
               /project/rlisia/consistency_hu1ca179bcb047f7a1fbab8caa7874e21b_35741_465f442911f10bfc7200508a17a3f094.png 760w,
               /project/rlisia/consistency_hu1ca179bcb047f7a1fbab8caa7874e21b_35741_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/rlisia/consistency_hu1ca179bcb047f7a1fbab8caa7874e21b_35741_4e41d84c91b3ce3cee113f3b57a93fe0.png&#34;
               width=&#34;760&#34;
               height=&#34;190&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Consistency comparison between GADLE and other algorithms
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Model Parallelism for Inference at edge</title>
      <link>https://prasang-gupta.github.io/project/modelparallelism/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/modelparallelism/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to create a solution that can be used to train a model that can be split into multiple pieces and can be run in parallel on multiple separate devices.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;














&lt;figure  id=&#34;figure-solution-architecture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Solution Architecture&#34; srcset=&#34;
               /project/modelparallelism/architecture_hub5f90f1de448dc6defbc2ca2b31e2b14_73424_215c88f36fa3407aa164683e7c3e7ca6.png 400w,
               /project/modelparallelism/architecture_hub5f90f1de448dc6defbc2ca2b31e2b14_73424_37006373126bde36e2cc0a8a4e097abb.png 760w,
               /project/modelparallelism/architecture_hub5f90f1de448dc6defbc2ca2b31e2b14_73424_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/modelparallelism/architecture_hub5f90f1de448dc6defbc2ca2b31e2b14_73424_215c88f36fa3407aa164683e7c3e7ca6.png&#34;
               width=&#34;760&#34;
               height=&#34;120&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Solution Architecture
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The architecture used was the recently published DeCNN (Decoupled CNN) network. The authors had used GConv with Channel Shuffle and added other network and OS level modifications to improve the performance of the decoupled architecture. For initial testing, we only focused on Scheme 1 and ignored other optimisations to establish a baseline.&lt;/p&gt;
&lt;p&gt;We used the tiny imagenet dataset and Resnet-34 model for testing and comparing the standard CNN results with DeCNN. We compared based on the performance (accuracy metrics) and the time taken to run inference on a fixed batch of data. It was found that there was a performance decrease by using DeCNN as some of the information is lost while doing channel shuffling. To counter that, we used 1.5x kernels as in the standard CNN to keep the performance comparable. Even while using just the Scheme 1 mentioned above, we got about 15% bump up in inference speeds with just 2 devices. We expect that this gap would increase with larger models and with more number of parallel devices.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution developed can be used to train models specifically for running on smaller edge devices. This is especially helpful as no single device can hold the full model in memory at the edge due to size and compute limitations and this would help in running bigger and better models at the edge with no added requirement of a heavy compute engine deployment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Read-The-Docs Documentation Templatisation</title>
      <link>https://prasang-gupta.github.io/project/rtddocumentation/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/rtddocumentation/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to create a solution that would auto-create documentation given a code repository with docstrings. The main focus of this solution was be to allow generating quick documentation that can be hosted on Github pages for making it easier to re-use codebases because of a standardised documentation format.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We created a documentation template and chose RTD (Read The Docs) as standard formatting because of its popularity among open source tools. We built 2 methods within the solution, first focused on generating and hosting documentation quickly (under 10 minutes) and the second focused on learning the nitty-gritties of how Sphinx works and providing much more room for modifications and personalisation. This system can also be used to generate documentation in PDF format using latex.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution was used widely across different teams and it allowed easy re-use of code. It also cultivated a good culture of writing docstrings in functions and classes across the whole team.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concept Drift Detection with Chatbots</title>
      <link>https://prasang-gupta.github.io/project/conceptdrift/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/conceptdrift/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to create a drift detection toolkit that can be easily used to detect drift in any type of data (image, audio or text) using a multitude of different drift detection methods to suit every problem under the sun. We also tested this toolkit&amp;rsquo;s ease of use by several case studies of different mock data types and also by integrating this with a chatbot built using RASA architecture to generate a novel drift-aware monitored chatbot.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The drift detection toolkit was built using several different drift detection methods like :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Drift Detection Type&lt;/th&gt;
&lt;th&gt;Drift Detection Methods&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Data distribution based methods&lt;/td&gt;
&lt;td&gt;- Kolmogorov-Smirnov (KS) test&lt;br&gt;- Maximum Mean Discrepancy (MMD) test&lt;br&gt;- Least-Squares Density Difference (LSDD) test&lt;br&gt;- KMeans and Chi Square Test&lt;br&gt;- Equal Intensity KMeans (EIKMeans) and Chi Square Test&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Drift magnitude based methods&lt;/td&gt;
&lt;td&gt;- Relative drift using Jensen–Shannon (JS) Divergence&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Uncertainty based methods&lt;/td&gt;
&lt;td&gt;- Uncertainty Classifier&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Error rate based methods&lt;/td&gt;
&lt;td&gt;- Fisher’s Test&lt;br&gt;- Statistical Test of Equal Proportions (STEPD)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We implemented these methods and verified that these methods are functioning properly using curated open-source datasets. After verifying all these methods for different types of data (text, audio and images), we implemented an integrated architecture with a chatbot built using RASA framework.&lt;/p&gt;














&lt;figure  id=&#34;figure-rasa-process-flow&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;RASA process flow&#34; srcset=&#34;
               /project/conceptdrift/architecture_hu33e64db526110706ea522e7294c7828c_88356_2a4979ed24c2fd3cbcdcf52825f299e9.png 400w,
               /project/conceptdrift/architecture_hu33e64db526110706ea522e7294c7828c_88356_478f1884e70c51c3c2bfed12ad677330.png 760w,
               /project/conceptdrift/architecture_hu33e64db526110706ea522e7294c7828c_88356_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/conceptdrift/architecture_hu33e64db526110706ea522e7294c7828c_88356_2a4979ed24c2fd3cbcdcf52825f299e9.png&#34;
               width=&#34;760&#34;
               height=&#34;440&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      RASA process flow
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The process flow diagram shows the working of the integrated system. We implemented several novel methods that ensured our solution remained as general as possible and it does not hamper the whole process in any way whatsoever. Additionally, in case drift is detected, we also made a training pipeline that would incorporate the changes in the model weights without having any model downtime.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution developed can be implemented with most of the chatbots that are currently in production. Implementing our system would ensure that the model does not lose intent quickly, and if it does, then proper notifications are being provided to the user based on the drift detection systems in place. It would also provide the developer with all the data that is needed to troubleshoot any issues and if needed, retrain the model to counter the drift without experiencing any downtimes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spatial  AI</title>
      <link>https://prasang-gupta.github.io/project/spatialai/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/spatialai/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this study was to test out some of the features offered by Worlds.io and create a generalised digital twin solution that can be leveraged in future projects. This would include generating a digital replication of their current environment and answer some of the questions based off of that with some additional analysis to allow clients to make better oeprational and strategic decisions.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;animation.gif&#34; alt=&#34;Solution Architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;We took the live footage provided by the vendor in New York City&amp;rsquo;s Bryant Park as a case study. A YOLO model was put on the raw camera footage to capture some common objects like people, bins, animals (pets) etc. This was then converted into a streaming data format and sent to cloud with approximate latitude and longitude values as provided by the vendor based on the camera&amp;rsquo;s location and field of view. These lat-lon values were then converted into cartesian coordinates, effectively treating Bryant park in a 2D plane for getting the data ready for digital twin studies. We also created a few zones and mapped the coordinates accordingly.&lt;/p&gt;
&lt;p&gt;The conversion to cartesian coordinates helped us to map the pedestrian movement patterns in the field of view. Additionally, we also extracted other information to answer questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How many people are passing through our zones in the park?&lt;/li&gt;
&lt;li&gt;How long are people generally in the park for?&lt;/li&gt;
&lt;li&gt;How many people are sitting in the park?&lt;/li&gt;
&lt;li&gt;How many people are walking babies/strollers?&lt;/li&gt;
&lt;li&gt;How many tourists/suitcases ?&lt;/li&gt;
&lt;li&gt;Are people riding bikes in the park?&lt;/li&gt;
&lt;li&gt;How many deliveries with carts are made?&lt;/li&gt;
&lt;li&gt;How many people are walking pets in the park?&lt;/li&gt;
&lt;li&gt;How many people per hour, walking speed, which way are they going,&lt;/li&gt;
&lt;li&gt;What % of people are stopping?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We were able to run analytics on the video feed and the derived variables. Also, we were able to generate a digital twin using the converted coordinates. This helped us build capability for any future projects that might entail the use of these technologies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HTML UI element extraction</title>
      <link>https://prasang-gupta.github.io/project/websiteuidetection/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/websiteuidetection/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$2^{nd}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this hackathon was to localise and identify several different HTML UI elements in hand-drawn wireframe drawings of websites as well as for screenshots of real websites.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset provided for the hackathon contained about 3200 wireframe drawings of websites. The goal was to identify the different HTML UI elemnents present in the image, such as &amp;ldquo;Text Box&amp;rdquo;, &amp;ldquo;Button&amp;rdquo;, &amp;ldquo;Image&amp;rdquo;, etc. Hence, it boiled down to an object detection problem. It also included another dataset containing screenshots of real websites. The problem remained the same for both the datasets.&lt;/p&gt;
&lt;p&gt;We used several different Object Detection techniques and decided on using the just released SOTA model YOLOv5. We tried different flavours of YOLOv5 and since inference time was not a bar, we went ahead with the XL version of the same to boost performance. We also used pre-trained weights and performed a LR scheduler study to boost the scores even further.&lt;/p&gt;
&lt;p&gt;We also observed that our model was giving out good predictions for the common elements with high confidences, but was not giving outputs for the more uncommon elements. Hence, we performed a study to change the confidence cutoff levels to optimise it for the use case and adjust it according to the distribution in the data. This allowed us to achieve near-perfect performance level of 0.95 F1 score on the validation set.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution built was performing really good on unseen test images managing an mAP value of 0.82. This model was later swapped with the last year&amp;rsquo;s model in the already developed pipeline to allow rapid prototyping of websites and dashboards.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automated PPT content editor</title>
      <link>https://prasang-gupta.github.io/project/yvce/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/yvce/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to generate a solution that would auto-format any PPT in PwC-compliant format. The changes included several editorial changes (word alternatives, punctuations) and branding changes (colors, formatting). It also included aligning any misaligned objects present in the PPT.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We decided to edit the underlying XML format of the presentation (using the open office lxml format) in addition to actually editing the PPT itself. We wrote code to parse the PPT in an editable format and then created a modular structure to work on different portions of the PPT. Some modules were rule-based, some were logic driven based on the requirements and some modules incorporated ML solutions developed for sub-problems wherever possible. Some of the modules built were:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Module&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Function and Details&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Word&lt;/td&gt;
&lt;td&gt;Editorial&lt;/td&gt;
&lt;td&gt;Performed changes on the word level. One was making them consistent in terms of American English / British English. Also included removing any risk words and replace jargons with better suited alternatives&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Numbers&lt;/td&gt;
&lt;td&gt;Editorial&lt;/td&gt;
&lt;td&gt;Performed date parsing, currency conversions etc based on the format expected. Also changed numerical numbers to text wherever applicable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Punctuation&lt;/td&gt;
&lt;td&gt;Editorial&lt;/td&gt;
&lt;td&gt;Added and modified punctuation marks wherever applicable in a consistent format&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Paragrapsh&lt;/td&gt;
&lt;td&gt;Editorial&lt;/td&gt;
&lt;td&gt;Identified any lengthy paragraphs or capitalisation issues in the presentation and gave suggestions to shorten it&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Font&lt;/td&gt;
&lt;td&gt;Branding&lt;/td&gt;
&lt;td&gt;Changed font sizes, styles and colors based on the location of the text (header, help box, content box etc)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bullets&lt;/td&gt;
&lt;td&gt;Branding&lt;/td&gt;
&lt;td&gt;Formatted simple and nested bullets to follow a particular pattern and adjusted font size and bullet marker according to the context&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Header and Footer&lt;/td&gt;
&lt;td&gt;Branding&lt;/td&gt;
&lt;td&gt;Adjusted header and footer in the master slide to be put on every slide in the document. Also, detected and accounted for repeated non-aligned headers and footers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pictogram&lt;/td&gt;
&lt;td&gt;Branding&lt;/td&gt;
&lt;td&gt;Detected images present in the slides and checked whether they are approved pictograms or if they infringe any copyright claims&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Colors&lt;/td&gt;
&lt;td&gt;Branding&lt;/td&gt;
&lt;td&gt;Changed the colors (font, background, pictograms) to be replaced with the nearest PwC-approved colors based off of a novel color matching technique&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Animations&lt;/td&gt;
&lt;td&gt;Branding&lt;/td&gt;
&lt;td&gt;Detected and changed any animations in the presentation wherever applicable&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This solution built was deployed on an internal hosting service and was made available as a service. The total processing time for an average presentation was about 5 minutes with all the modules active. This brought down the time to manually review and format average presentations from 30 minutes to about 10 minutes. The solution was not perfect, but it helped ensure that most of the repetitive tasks are taken care of by the code and only final inspection with some modifications need to be done by the manual reviewers, saving thousands of manhours for the firm.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Twitter Bias Hackathon</title>
      <link>https://prasang-gupta.github.io/project/twitterbiashack/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/twitterbiashack/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$9^{th}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of the hackathon was to higlight bias in Twitter&amp;rsquo;s Saliency model.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Twitter&amp;rsquo;s Saliency model is a model that is used to crop over-sized images to fit on the screen representing a thumbnail preview. The saliency model is responsible for selecting the most &amp;ldquo;salient&amp;rdquo; part of the image and that part is consequently kept in the thumbnail and the area surroinding it is cropped off.&lt;/p&gt;
&lt;p&gt;We decided to chose, recently concluded at the time, Tokyo Olympics 2022 as a case study for highlighting bias. We downloaded around 5000 images across 10 different sports and ran the saliency model on top of them. We also passed these images through an object detection model to identify images with and without athletes.&lt;/p&gt;
&lt;p&gt;We ran several studies, the main being classifying the images and the crops as something that was biased or not-biased. We also ran a sensitivity analysis and also tried observing changes between original coloured images and an image with a filter put on it (sepia and grayscale).&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We found out that the model, in some cases, was biased towards text present in the image (which was mostly Tokyo 2020). We also found in several cases, where the main focus of the image, the athletes were not the ones selected as the most salient in the image, instead, the saliency model was predicting either someone from the audience or billboards / advertisements as the most salient. This report was submitted to Twitter and we got a thanks from the Twitter team for highlighting this bias in their model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distributed Edge Compute</title>
      <link>https://prasang-gupta.github.io/project/distributededge/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/distributededge/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this study was to build a system to distribute compute load across different resources present on the edge. It also involved testing the built system with a dummy exercise and find out the robustness of the system and any additional points that might need to be taken care of before actually attempting a client project in this domain.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;














&lt;figure  id=&#34;figure-solution-architecture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Solution Architecture&#34; srcset=&#34;
               /project/distributededge/architecture_hu9a6a3449283a00e41d8fa0a852df1805_67176_41b426d29e42d56969afa81be83afec6.png 400w,
               /project/distributededge/architecture_hu9a6a3449283a00e41d8fa0a852df1805_67176_3abc4eba1fa3725797fb60596476dc0a.png 760w,
               /project/distributededge/architecture_hu9a6a3449283a00e41d8fa0a852df1805_67176_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/distributededge/architecture_hu9a6a3449283a00e41d8fa0a852df1805_67176_41b426d29e42d56969afa81be83afec6.png&#34;
               width=&#34;669&#34;
               height=&#34;406&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Solution Architecture
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;We built out a system for distributed edge compute using microk8s, a micro kubernetes engine. We paired it with metalb, a load balancer and dashboarding, resource tracking and reporting tools like grafana and prometheus. To simulate an edge environment, a local ensemble of devices was created and connected together on the same network. A total of 3 devices were included in the network including a raspbery pi, a windows laptop and an edge compute appliance. For testing out the pipeline, we ran a simple object detection model on all the devices using tensorflow serving.&lt;/p&gt;
&lt;p&gt;While testing, we also came across several limitations of the architecture. The first is that the compute power was very skewed within the networks (x86 devices were orders of magnitude faster than ARM). This lead to bottlenecking at times even with the load balancer. We also realised that running a full object detection model on small devices was not a great idea due to the compute and memory limitations of each device.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The system built was a demonstration that multiple unused devices can be pooled together at the edge to increase the overall compute capabilities, reducing the lag of transferring data to and from the cloud. Apart from reducing latency, it also helps in keeping the data secure as everything is happening locally. This also ensures that no single edge device is overutilised hampering the pipeline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Occupancy Detection</title>
      <link>https://prasang-gupta.github.io/project/occupancydetection/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/occupancydetection/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;PROBLEM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The client wanted to prepare for and ensure a safe transitioning of people from &amp;ldquo;Work from home&amp;rdquo; to Office space. They wanted to make sure that the facility should be used responsibly and at no time there should be breaches of the social distancing regulations.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;SOLUTION&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;














&lt;figure  id=&#34;figure-solution-architecture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Solution Architecture&#34; srcset=&#34;
               /project/occupancydetection/architecture_hu7a7b226e80ad51eb45f98babb4f1dc5f_821435_92d75e78e9ad497b878f15d448669dbc.png 400w,
               /project/occupancydetection/architecture_hu7a7b226e80ad51eb45f98babb4f1dc5f_821435_24505b515ffc5278de06819714ae5209.png 760w,
               /project/occupancydetection/architecture_hu7a7b226e80ad51eb45f98babb4f1dc5f_821435_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/occupancydetection/architecture_hu7a7b226e80ad51eb45f98babb4f1dc5f_821435_92d75e78e9ad497b878f15d448669dbc.png&#34;
               width=&#34;749&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Solution Architecture
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;We prepared a solution that gave the client the ability to track and measure the occupancy and social distancing norms anonymously. We set up a LIDAR in the client office space attached with a hub to send the data to the cloud. Several calculations and checks were performed on the cloud and the final data was sent to the dashboard. The real time occupancy readings (both overall and zone-based) were also visualised by the dashboard.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As a result of our solution, the client could monitor occupancy in real-time, track occupancy trends and monitor the hotspots in the office. As an extension, this can further be used to optimise the office space usage.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Demand Prediction with Competition Analysis</title>
      <link>https://prasang-gupta.github.io/project/demandprediction/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/demandprediction/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this study was to predict consumer demand using data from competition and infusing it with other datasets that influence demand like mobility and demographic variables. The other goal was to correct a proprietary mobility dataset used by PwC to incorporate for mobility changes brought on by COVID restrictions.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proprietary dataset mentioned had several flaws including data inconsistencies over months and years and noisy data which was not helping in developing stable models. It also didn&amp;rsquo;t account for any socio-economic variables based on the demographics of the region. To correct this, the proprietary dataset was merged with demographics variables based on the location and some feature engineering was done to get the mobility in radii of 1km to 5km based on the store location. The final sales value was kept as the dependent field for the model.&lt;/p&gt;
&lt;p&gt;Multiple model architectures were tried to model this information to correct the mobility, including ML and DL models. Finally, a voting ensemble method was selected which was modelling the corrected demand with an accuracy of about 70%.&lt;/p&gt;
&lt;p&gt;To bump up the numbers further, an aerial snapshot of the region around the score was obtained from Google Satellite API at a tuned zoom level and a segmentation exercise was performed on the image to extract the type of region around the store. This was expected to bring another dimension of immediate demographics in the dataset. This was done by extracting the coverage area of different colors based on pixel masks :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grey (signifying roads, parking lots, streets, etc)&lt;/li&gt;
&lt;li&gt;Green (signifying agriculture, parks, vegetation, etc)&lt;/li&gt;
&lt;li&gt;White (signifying roof-tops, buildings, etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The final model with all the variables included (engineered visits from proprietary data, Google mobility data, demographics data and derived immediate demographics of the store from satellite images) achieved a test accuracy of 75%. This improved the quality of the mobility data and impacted tens of projects that utilised this mobility dataset helping in building much more accurate and robust models for client deliveries.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Factory Intelligence</title>
      <link>https://prasang-gupta.github.io/project/factoryintelligence/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/factoryintelligence/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;PROBLEM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Our client was facing unplanned machine downtimes in their factories. Obtaining insights from machinery on factory floors is time-consuming, complex, and costly due to legacy infrastructure and bespoke systems that cannot be easily accessed. Accelerating the time it takes to draw intelligence from machine data is critical to get an accurate pulse on manufacturing operations. While many vendors offer comprehensive IIoT solutions, we were looking for quicker and more cost-effective alternatives to deploying a sophisticated end-to-end platform.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;SOLUTION&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;














&lt;figure  id=&#34;figure-solution-architecture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Solution Architecture&#34; srcset=&#34;
               /project/factoryintelligence/architecture_huf2eca023e3a5b6f297102972273e9a3e_1060700_ebf303ea2be1037a783bdc8fcd762a18.png 400w,
               /project/factoryintelligence/architecture_huf2eca023e3a5b6f297102972273e9a3e_1060700_0e2693532b5456e754cbd13354aeb822.png 760w,
               /project/factoryintelligence/architecture_huf2eca023e3a5b6f297102972273e9a3e_1060700_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/factoryintelligence/architecture_huf2eca023e3a5b6f297102972273e9a3e_1060700_ebf303ea2be1037a783bdc8fcd762a18.png&#34;
               width=&#34;760&#34;
               height=&#34;508&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Solution Architecture
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;We designed, deployed and tested a fully-functioning prototype on our client’s manufacturing floor in less than 4 weeks. Our solution utilized low-cost vibration sensors and accelerometers connected directly to selected machines. These sensors captured near real-time data and provided rapid analytics by bypassing timely integrations with existing factory systems.&lt;/p&gt;
&lt;p&gt;We also designed and built a customized dashboard offering a simple and elegant view of the captured insights. The insights present on the dashboard included machine schedules, unplanned downtimes, overall availability and the number of times the machine was stopped. All of this was reported and updated real-time with a resolution of 4 minutes.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Instead of spending months and significant budget to deploy an end-to-end IIoT platform, our client was equipped with critical insights in a much shorter timeframe. KPIs captured included machine up/downtime compared with scheduled operating times. The intelligence gathered not only provided efficiency gains and cost savings, but also avoided our client complex work orders to deploy sensory equipment. Consequently, client involvement was low-touch and the manufacturing process was not interrupted.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L2RPN Hackathon 2020 - Robustness Track</title>
      <link>https://prasang-gupta.github.io/project/l2rpn/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/l2rpn/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$28^{th}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt; (&lt;span style=&#34;color:#33cc33;font-style:bold;font-size:100%&#34;&gt; $21^{st}$ highest score&lt;/span&gt; )&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of L2RPN (Learning to Run a Power Network) 2020 hackathon - Robustness Track was to build a robust agent that would be keep delivering reliable electricity everywhere and also operate the grid safely when an agent takes unknown adversarial actions at regular intervals.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset contained episodes spanning different adversarial actions taken by the agent. The agent could terminate any 1 of the 10 possible power lines (some of them being high voltage lines). Our agent was to be evaluated on how long it can provide reliable power to consumers without causing blackout. Once a blackout occurs, it is game over for that episode. The operation cost to be minimized included powerline losses, redispatch cost and blackout cost.&lt;/p&gt;
&lt;p&gt;Every substation in the competition grid had a &amp;ldquo;double busbar layout&amp;rdquo;. Hence, there was a choice of bus for making a connection from one of the bus to a grid object. Due to this and the medium sized grid used, the action space was of the order of 10^5 with the combinatorial action space reaching infinity.&lt;/p&gt;
&lt;p&gt;Our approach was derived from a previous &lt;a href=&#34;https://github.com/ZM-Learn/L2RPN_WCCI_a_Solution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;public solution&lt;/a&gt; for this competition. THe idea was to reduce the action space and train 2 A3C models with different training parameters.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We managed to improve upon the baseline set and managed to achieve a score of 10.84, which was the $21^{st}$ highest score on the leaderboard, gaining us $28^{th}$ rank.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fingervein Detection</title>
      <link>https://prasang-gupta.github.io/project/fingervein/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/fingervein/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to build out a fully functional proof of concept for the fingervein detection biometric system. This end-to-end system would include a frontend for new user registration, verification of user fingervein and deletion.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We achieved this using a Fingervein device and a custom built secure backend database. All the services were defined as part of an API which was hosted on a cloud platform. All the information transfer was done after using encryption. Furthermore, the frontend was built as a Google Chrome extension and integrated right into the browser for easy usage. This would ensure scalability and hassle free adoption.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We were able to test out the full end-to-end pipeline right from registering new users and other management options provided to actually verifying the users based on the fingervein scans. This capability was built out and documented to be used for any future client engagements as a replacement for the traditional fingerprint security system.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HTML UI element extraction</title>
      <link>https://prasang-gupta.github.io/project/wireframeuidetection/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/wireframeuidetection/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$3^{rd}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;in the hackathon.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this hackathon was to localise and identify several different HTML UI elements in hand-drawn wireframe drawings of websites.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset provided for the hackathon contained about 3000 wireframe drawings of websites. The goal was to identify the different HTML UI elemnents present in the image, such as &amp;ldquo;Text Box&amp;rdquo;, &amp;ldquo;Button&amp;rdquo;, &amp;ldquo;Image&amp;rdquo;, etc. Hence, it boiled down to an object detection problem.&lt;/p&gt;
&lt;p&gt;We tried using several Object Detection algorithms like YOLO, R-CNN and Mask-RCNN and decided on Mask-RCNN as it was providing us with the best results. However, one thing we observed in our outputs was that our Precision scores were good, but the model was lacking in Recall bringing the whole F1 down. To solve this problem, we came up with a novel technique &amp;ldquo;Multi-Pass Inference&amp;rdquo; that booosted our recall scores.&lt;/p&gt;
&lt;p&gt;The technique involves running the image through the model multiple times, each time taking note of the objects that are already detected and removing them for subsequent passes. This forced the model to predict more instances of the elements in the image. We smarlty combined the objects detected in multiple passes to overall boost the recall score of our model helping us to take a podium spot in the leaderboard.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution built was performing really good on unseen test images managing an mAP (IoU &amp;gt; 0.5) score of 64.12. This solution was later implemented into a pipeline to allow rapid prototyping of websites and dashboards.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cross-country Asset Tracking</title>
      <link>https://prasang-gupta.github.io/project/assettracking/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/assettracking/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;PROBLEM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The client were facing problems with cargo damage while in transit which were resulting in huge losses for them. Also, since the transit included multiple contractors responsible for different stretches of the overall route, and nobody was taking responsibility for the damages, they were at a loss and had no idea how to rectify this.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;SOLUTION&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;














&lt;figure  id=&#34;figure-solution-architecture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Solution Architecture&#34; srcset=&#34;
               /project/assettracking/architecture_hue54687bcbd9ff4b5b81b1383dd755f5a_351565_19ea7e81f4067304a3e99ad795d6d8c0.png 400w,
               /project/assettracking/architecture_hue54687bcbd9ff4b5b81b1383dd755f5a_351565_1c715ca36d1709ba4205a7e9a925488c.png 760w,
               /project/assettracking/architecture_hue54687bcbd9ff4b5b81b1383dd755f5a_351565_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://prasang-gupta.github.io/project/assettracking/architecture_hue54687bcbd9ff4b5b81b1383dd755f5a_351565_19ea7e81f4067304a3e99ad795d6d8c0.png&#34;
               width=&#34;760&#34;
               height=&#34;326&#34;
               loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Solution Architecture
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;We created an end-to-end asset tracking system for the client. The clients were given low-cost devices housing different sensors to include them with their shipments. These devices were very small (about 2cm x 2cm x 1cm) and were tested for harsh weather. Also, these devices were equipped with a battery, a GPS and cellular communication technology along with the accelerometer, humidity and temperature sensors. These were configured to be used as one time use devices to save on device recovery costs / logistics. They were configured to report information all along the journey and last the whole shipment time without the need of a recharge.&lt;/p&gt;
&lt;p&gt;Thresholding was done on the accelerometer sensor housed in the device to store high impacts faced by it, which would directly correlate with the damage faced the equipment. These raw accelerometer values were then configured to be sent to the cloud managed by the manufacturer. This data was then available to be used using simple API requests.&lt;/p&gt;
&lt;p&gt;The raw data collection and conversion to impact system was built on an application enablement platform. This resulted in quick prototyping and testing of the proof of concept. The processed data was then sent to a live dahsboard which was shared with the client for their perusal. This dashboard offered several important metrics such as the last known location, temperature and battery of the device. It also showed the impact values versus timestamp which were recorded by the device.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From using this solution, we were able to ascertain different impact values faced by the device during different portions of the journey. This helped the client to visualise and piece together possible regions where the shipment was getting damaged. Our analysis found that most of the impacts registered by the device were when the device was travelling from the factory to the ports, which was contrary to the earlier belief of the client that most of the damage was being caused during the ship route. This allowed the client to focus on that section and helped them save a lot of revenue spent on replacing damaged products as well as save the reputation of the company.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Trainings Repository</title>
      <link>https://prasang-gupta.github.io/project/lndrepo/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/lndrepo/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was to consolidate the different trainings held in the firm for the last 3-4 years and generate a searchable dashboard with relevant information.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The consolidation and standardisation for all the trainings was done by scraping information from mails and other portals and arranging them in google drive based on the scraping output. A live tableau dashboard was then generated from the index sheet and deployed on a central platform that everyone in the firm can access. Apart from keyword search, multiple filters were also included in the dashboard.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This dashboard allowed people in the firm to search for past trainings with feedback and help plan future trainings based on that. Also, several hours were saved allowing people to re-use decks and other material from past trainings. This also helped foster an environment of self-learning within the firm as one can just search and get all the material relevant for a particular topic in just seconds.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active Learning</title>
      <link>https://prasang-gupta.github.io/project/activelearning/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/activelearning/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of the study was to build out a pipeline for active learning that other projects with scarce labelled data will leverage for model building. It also included trying the effectiveness of active learning on a standard dataset and evaluate different techniques.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The dataset chosen for this study was CIFAR10. This dataset contains 60,000 32x32 images with 10 unique classes. We used a maximum of 10,000 images at each time for training purposes. This study&amp;rsquo;s focus was to compare the performance of active and passive learning using same number of labelled samples.&lt;/p&gt;
&lt;p&gt;The passive learning model would be trained on a uniform distribution of the dataset size, however, the active learning model would be initialised with half the training size and then would be trained for the full size by sequential queries. We tried 3 queries in this study : uncertainty, margin sampling and entropy sampling. We achieved better performance with active learning for all the sizes, however, the improvement in performance varied from as low as 1% to a sizable 23%. The details for all the experiments and the queries can be seen in the slides.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The pipeline developed from this project was consecutively used in multiple client projects to improve the modelling capabilities and thrive wherever manually labelling the full dataset was not an option.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implementation of live models on edge</title>
      <link>https://prasang-gupta.github.io/project/edgelivemodels/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/edgelivemodels/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The aim of this project was two fold. The first aim was to implement an action recognition model and the second was to modify it to run on an edge device. For this, we chose the Pre-trained Temporal Relation Network Model. The dataset chosen for this was the 20BN-something-something Dataset V2. This dataset has over 100 classes of different object-human or object-object interactions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;jetson.png&#34; alt=&#34;Jetson TX2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The model was first implemented on a laptop with the webcam and then later extended onto the edge device, Jetson TX2. Prior to this, the TX2 was flashed and proper libraries were built from source to enable it to use its full potential (CUDA cores for rendering). We were successfully able to implement this on the board and were getting very respectable frame rates, somewhere around 10 fps. The performance of this model for several different scenarios can be seen in the videos link and a little detail about the implementation can be found in the slides.&lt;/p&gt;
&lt;p&gt;Apart from this, we also implemented simple object detection models on Raspberry Pi on which we were getting around 1-2 fps.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These implementations paved the way for future projects that involved hosting models on smaller edge devices. These capabilities were built for the first time within the team.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Damaged Car Parts Segmentation for auto claims</title>
      <link>https://prasang-gupta.github.io/project/carpartsegmentation/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/carpartsegmentation/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;PROBLEM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The client wanted to reduce the time spent by their employees in looking through several different photographs submitted for insurance claims clearing and ascertain damaged parts of the vehicle with the extent of damage.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;SOLUTION&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We solved the problem by training a semantic segmentation model used for ascertainining the different kinds of damage that were present in the photograph of a vehicle (like the figure attached). To ensure the correctness of the model, we also employed an explainable AI technique, LIME, which returned the parts of the image it is looking at when coming to a decision about the damage of a particular type.&lt;/p&gt;
&lt;p&gt;A few classification models were also trained to fetch images which were visually similar with the current image. The different variants were the similar damage model, where the model would return the top images which have visually similar damage to the current image. The other variant was the similar non-damage model, in which the model would return the top images of similar vehicles of previously processed claims.&lt;/p&gt;
&lt;p&gt;To top it all off, an automated report generation tool was coupled with the model (using FPDF), which returned a formatted PDF report having detailed information regarding the damage and the claims (a sample report attached).&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The segmentation model ensured that the client team spent lesser time on figuring out the damage and more time providing personalised support to the consumers. The classification models helped the client team to look at some previous claims to decide the outcome for the current claim in a more informed and consistent manner. This improved the overall reputation of the firm in disbursing out claims. Also, the automated report could be handed over to the consumers directly for a much more transparent view into the claims processing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Complete Plant Design of Acrylic Acid</title>
      <link>https://prasang-gupta.github.io/project/che453/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/che453/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This was a course project for Chemical Desgin course under Prof. Nitin Kaistha. The aim of the project was to design a chemical plant from scratch for the production of acrylic acid. The design should be optimal with respect to the different costs associated with the production and it should be robust to input stream fluctuations. This was a rather interesting project as it brought together everything that we learned in the span of 3 years.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The design was meticulously thought of by theoretical knowledge and the whole plant was simulated using ASPEN HYSYS package. The simulations were done both for the equilibrium stage of the plant as well as for the dynamics with controllers attached to several parts of the components, as can be seen in the figure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Macroscopic model for Transdermal Drug Delivery</title>
      <link>https://prasang-gupta.github.io/project/transdermal/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/transdermal/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This project was part of my internship at TATA Research Design and Development Centre. The internship started May 2017 and ended July 2017. The objective of the project was to develop a macroscopic model of skin’s upper layer stratum corneum using OpenFoam, an open-source library. This library was chosen as this provided and open source FVM alternative to the licensed FEM solver, COMSOL. Apart from that, nobody in the firm had used it before, so it served as an exploration study into using this library. However, as I was able to finish the objective early, I also finished an extension of the project, which included studying the effects of electroporation on the drug transfer kinetics.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The first task was to get used to the OpenFOAM library which is completely CLI based. After getting a handle of the different methods and way of working, the first model was developed of the uppermost skin layer, Stratum Corneum using a parameterised brick and mortar structure. This model was then used to model a transdermal drug delivery by Fickian diffusion through the layer.&lt;/p&gt;
&lt;p&gt;Fentanyl and Caffeine were chosen to be studied based on literature review and their scope of usage in transdermal delivery. The diffusion coefficients for these components were obtained from MD simulations. Running them through the developed model provided drug release profiles for both. These profiles were validated by experimental studies present in literature.&lt;/p&gt;
&lt;p&gt;Since, there were a lot of files that needed to be modified to set up an OpenFOAM simulation, running iterations while maintaining correctness (not missing editing a file) was a steep task. Hence, I worked on automating the whole simulation process and file editing structure using C++ and Bash. This reduced simulation set up times drastically allowing me to run several iterations and complete the objective of my internship way before my internship end date.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I was able to extend the study to include the effects of electroporation on the drug delivery profiles. The new model developed with electroporation was again validated with experimental data and was found to be in good agreement. Also, the drug release significantly increased after using electroporation which was expected from the study.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Numerical Study of SiO2 Burner profiles</title>
      <link>https://prasang-gupta.github.io/project/burnerprofile/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/burnerprofile/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This was an Undergraduate project under Prof. Naveen Tiwari, Prof. V Shankar and Prof. Goutam Deo from January 2017 to March 2017. The aim of the study was to model an SiO2 burner with specific dimensions and study the temperature profiles of the burner. The project also included studiyng the distribution of different species present at different distances away from the flame.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To kick off the modelling, adiabatic flame temperature was calculated for a hydrogen-oxygen combustion process. The equilibrium analysis was done on ASPEN Plus package. To validate the results, the same value was cross-referenced with literature values. It was found to be in good agreement with that. Next, a 100% conversion analysis was done using the same suite. However, this time the same model was built using first principles on MATLAB and the results were compared. Again, there was a very good agreement with both of these values. Hence, we concluded that the modelling process done in the ASPEN Plus suite is correct.&lt;/p&gt;
&lt;p&gt;Having validated our model, we started experimenting with different conditions for the same process. This included changing the temperature as well as adding Silicon Tetrachloride in the feed. Finally, a dependence between the hydrogen flow rate with the adiabatic temperature was established.&lt;/p&gt;
&lt;p&gt;Next, we moved on to the kinetics of the problem. The overall reaction was broken down into several different intermediate reactions. Every one of these intermediate reactions have different kinetic parameters. All the possible intermediate reactions were written and validated with theory and literature. Preliminary reaction kinetics were performed with an assumed value of burner diameter and were qualitatively checked with literature values.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Having a validated model ready for use, the original burner dimensions used by the industrial partner were added to the model and all the calculations were re-run for this. The mole fractions of the different species were studied at the end of the flame. This provided much better view into the temperature variation in the flame and the different reaction progress with the length of the flame. This gave the industrial partner insights into the minimum length of the flame to ensure that the reaction gets over and a maximum conversion of raw materials to products can be achieved with minimal waste.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Numerical Investigation of Nanoparticle Precipitation in Y-Mixers</title>
      <link>https://prasang-gupta.github.io/project/ymixer/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/ymixer/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This was an Undergraduate project under Prof. Nishith Verma which went on from March 2016 to December 2016. Our aim was to perform an investigation into the particle densities obtained by mixing two fluids in a Y-shaped mixture.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As a first step, the y mixer model was generated in C++ in 2 and later 3 dimensions and simulations were run using the Lattice Boltzmann method. This resulted in fluid flow diagrams which were matched with experiments. After validating this model, tracer analysis was done on the outputs. This ensured us to capture the cross section and the mixing capabilities of the mixer. This was again validated from experiments.&lt;/p&gt;
&lt;p&gt;Having a C++ lattice boltzmann model ready, we moved on to using the palabos library to include population balance equations in our simulation model. This included the new calculations as well as sped up the entire code by about 40%. Nucleation rates and growth rates for the different particles were provided based on experimental chemical reactions and the population density of the product was studied at different cross sections of the mixer representing varying mixer sizes.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With this study, drug flow could be simulated in the blood vessels with proper mixing patterns, as in the case of y mixer. This would help in accelerating the drug flow studies as results could be obtained in a much faster way and the properties of the drug can be tweaked and adjusted based on the simulated flow patterns.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PID Characteristics</title>
      <link>https://prasang-gupta.github.io/project/innovationlabpid/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/innovationlabpid/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This was a part of the Unit Operations Lab course. The aim of this project was to innovate and go above and beyond the standard lab experiments with the same equipment.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We modelled the whole process in MATLAB Simulink deriving transfer functions from experimental data. Also, we took the default PID values present in the controller on the setup and tuned those values. We found that the response received is much better than the default response recorded. Hence, we submitted the parameters to be updated in the controller for more accurate experiments thereafter.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Composite Allegro 2m glider</title>
      <link>https://prasang-gupta.github.io/project/glidermentor/</link>
      <pubDate>Fri, 01 Jul 2016 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/glidermentor/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This was a voluntary project under the Aeromodelling Club in the summer holidays from May 2016 to July 2016. The aim of the project was to train a team to build a composite glider as well as try a new kind of HLG glider, the Allegro 2m.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As mentioned, the composite glider &amp;ldquo;Allegro 2m&amp;rdquo; was chosen for this project. The different airfoils were directly obtained from Prof. Mark Drela&amp;rsquo;s page and templates were cut using laser cutter. These templates were then used to cut the foam which would form the base of the wings / tails using a gravity hot wire cutter. These foam cuttings were then covered with an optimal number of glass fibre and carbon fibre sheets for providing it hte strength it needs. The optimum layers were calculated by designing a MATLAB code to take into account shear stress at different locations of the wing while in flight.&lt;/p&gt;
&lt;p&gt;Contary to the previous glider that was developed, this glider was also polyhedral but there was no dihedral in the middle and there were 2 angles on each side totalling 4 dihedrals. This ensured much better construction as giving a dihedral in the middle is very tricky. These were carefully built to maintian the same angle to ensure roll stability. The hollow fuselage was developed from the previously developed moulds. The hollow tail was built using cross arranged rolled carbon fibre cloth. Lastly, the wings and tails were spray painted and then all of the components were assembled together.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This project created a trend of using composite materials for fabrication in the Club paving way for future projects like &amp;ldquo;Composite Flying Wing&amp;rdquo;, etc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Centrifugal Pump Characteristics</title>
      <link>https://prasang-gupta.github.io/project/innovationlabpump/</link>
      <pubDate>Fri, 01 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/innovationlabpump/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This was a part of the Unit Operations Lab course. The aim of this project was to innovate and go above and beyond the standard lab experiments with the same equipment. We were assigned the centrifugal pump setup which is originally used to study the effect of series and parallel setup of pumps on the discharge pressure and flow rate. We however decided to perform a very different study using the same equipment.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;During our experiments, we noticed that there was a measurable gap between turning up the RPM of the motor and actually observing differences in the flow rate. This lead us to believe that there is some sort of a lag in play between the inputs and outputs. To model this lag, we assumed first order dynamics for simplicity and calculated the transfer function of the pump ny oberving the time difference it takes to reach equilibrium after giving the input. We later tested this obtained transfer function by further experimenting with the setup.&lt;/p&gt;
&lt;p&gt;To ensure utmost accuracy in our calculation, we used high FPS cameras to record readings when we were giving a small step change as an input to the motor (change of RPM). We also performed an opimisation study for an optimum RPM for the motor for best energy efficiency.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ABU Robocon 2016</title>
      <link>https://prasang-gupta.github.io/project/robocon16/</link>
      <pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/robocon16/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$3^{rd}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;among the 105 teams that participated from different technical institutions from all across India.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ROBOCON is an Asia-Pacific robotics competition held every year with different problem statements. The problem statements are carefully curated to encourage design innovations in the field of robotics. The theme of the competition for the year 2016 was &amp;ldquo;Clean Energy recharging of world&amp;rdquo;. The problem statement was to make 2 robots, one being a powered robot and the other being a non-powered robot that will move using non-contact forces provided by the former. The aim was to make the non-powered robot travel on a path with elevation, curves, sharp turns etc autonomously using line following. This bot was carrying a lightweight fan. The other robot, in addition to providing the required force for the non-powered robot to move, had to collect the aforementioned fan from the non-powered robot after it had covered the entire stretch, climb a pole and then place the fan on the top of the pole. This would mark the completion (&amp;ldquo;Chai-Yo&amp;rdquo; as they called it) of the challenge and the team that achieves this first would win.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I was one of the leaders in the 21 member team that took part in the competition represting my alma mater, IIT Kanpur. We worked on this from October 2015 to March 2016. We managed to achieve  that participated from different technical institutions from all across India.&lt;/p&gt;
&lt;p&gt;We chose to go with wind as the mode of force to move the non-powered robot. We used a propellor fan to generate the wind and a styrofoam sail was attached to the non-powered robot to catch the wind and propel it forward. This bot was made using lightweight balse wood and glass fibre which have an impressive strenght-to-weight ratio. We further designed the truss patterns on the wood and got it cut by laser to have the lightest possible bot. We also employed line tracking on the bot to follow the required path. These prototypes were first designed on Autodesk Inventor or Solidworks and then cut and assembled using precise machinery.&lt;/p&gt;
&lt;p&gt;The powered bot had an autonomous climbing mechanism using rubberised clamp wheels. To follow the path and align itself to the pole, it used wall following to follow the arena wall. We were consistently able to achieve &amp;ldquo;Chai-Yo&amp;rdquo; in about 50 seconds of runtime (You can take a look at the video which has the complete run while we were still preparing for the competition).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;award.JPG&#34; alt=&#34;Podium Finish at Award Ceremony&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Composite Aegea 2m glider</title>
      <link>https://prasang-gupta.github.io/project/gliderself/</link>
      <pubDate>Wed, 01 Jul 2015 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/gliderself/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This was a voluntary project under the Aeromodelling Club in the summer holidays from May 2015 to July 2015. The aim of the project was to build a composite glider.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The composite glider &amp;ldquo;Aegea 2m&amp;rdquo; was chosen for this project. The different airfoils were directly obtained from Prof. Mark Drela&amp;rsquo;s page and templates were cut using laser cutter. These templates were then used to cut the foam which would form the base of the wings / tails using a gravity hot wire cutter. These foam cuttings were then covered with an optimal number of glass fibre and carbon fibre sheets for providing it hte strength it needs. The optimum layers were calculated by designing a MATLAB code to take into account shear stress at different locations of the wing while in flight.&lt;/p&gt;
&lt;p&gt;The wings of the aegea were polyhedral with a dihedral in the middle and 1 more angle on each side totalling 3 dihedrals. These were carefully built to maintian the same angle to ensure roll stability. The hollow fuselage was developed from moulds which were again in turn made from composite materials. The hollow tail was built using cross arranged rolled carbon fibre cloth. Lastly, the wings and tails were spray painted and then all of the components were assembled together.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;IMPACT&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This was the first time that composite materials were being used in the Aeromodelling Club, IIT Kanpur. This project opened up a whole array of opportunities for the club to divest into composite construction of other designs including flying wings, UAVs etc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Semi-Autonomous Surveillance and Tranportation Robot</title>
      <link>https://prasang-gupta.github.io/project/sastr/</link>
      <pubDate>Wed, 01 Jul 2015 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/sastr/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This project was done as a part of the summer project offering from Robotics Club, IIT Kanpur. The aim of this project was to build an all-terrain robot capable of autonomous navigation and provide surveillance in remote areas.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We modelled the robot on Solidworks and machined it using aluminium and CNC machines. For navigation, we integrated the IMU sensor with the GPS anc coupled them together iwth a Kalman filter. Also, we imported Google Maps API for setting the route to be travelled and later converted the route into different waypoints to be followed. This waypoint file was uploaded to the robot&amp;rsquo;s APM (mission planner).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ABU Robocon 2015</title>
      <link>https://prasang-gupta.github.io/project/robocon15/</link>
      <pubDate>Sun, 01 Mar 2015 00:00:00 +0000</pubDate>
      <guid>https://prasang-gupta.github.io/project/robocon15/</guid>
      <description>&lt;p&gt;&lt;span style=&#34;color:#33cc33&#34;&gt;We achieved&lt;/span&gt; &lt;span style=&#34;color:#33cc33;font-style:bold;font-size:120%&#34;&gt;$11^{th}$ rank&lt;/span&gt; &lt;span style=&#34;color:#33cc33&#34;&gt;among the 80 teams that participated from different technical institutions from all across India.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;AIM&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ROBOCON is an Asia-Pacific robotics competition held every year with different problem statements. The problem statements are carefully curated to encourage design innovations in the field of robotics. The problem statement for the year 2015 was to make 2 robots that will play a doubles badminton match with the other team&amp;rsquo;s robots. The choice of fully automated vs manual robots was provided by the organisers. I was a part of the 21 member team that took part in the competition represting my alma mater, IIT Kanpur. We worked on this from October 2014 to March 2015.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-style:bold;font-size:120%&#34;&gt;&lt;a class=&#34;mt-1&#34;&gt;DETAILS&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The 2 bots designed were capable of serving the shuttle cock across the net to land in a specific region designated as a &amp;lsquo;successful serve&amp;rsquo; by the organisers. They also had mechanisms to return the serve of the opposing team using different placements of badminton racquets. The image shows one of our robots that had a double pneumatic actuation mechanism in the center alongwith 2 side racquets to return the shuttle.&lt;/p&gt;
&lt;p&gt;I was a part of the electronics team, managing the power delivery to the different components of the bot. Also, to reduce the clutter of wires, I designed Printed Circuit Boards (PCBs) of different components of the bot, eg. the motor driver, on Altium Designer and got them printed for use. I also had major contributions in designing the mechanism to serve the shuttlecock.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
